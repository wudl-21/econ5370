{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107b624",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rm(list = ls(all=TRUE)) # remove all objects\n",
    "Sys.setlocale(\"LC_ALL\", \"zh_CN.UTF-8\") # set locale to Chinese\n",
    "setwd(\"./\") # change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8684a",
   "metadata": {},
   "source": [
    "# Data Reading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc808627",
   "metadata": {},
   "source": [
    "## Primitive Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619c311",
   "metadata": {},
   "source": [
    "### Stock Return\n",
    "\n",
    ">  Monthly, between the first and last *trading day* of the month (not necessarily beginning and end of the month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d449532",
   "metadata": {},
   "source": [
    "- Read stock return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d04f38",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the stock data using read.csv\n",
    "stock <- read.csv(\"stock.csv\", header = TRUE, stringsAsFactors = FALSE, check.names = FALSE, colClasses = c(stkcd = \"character\", year = \"integer\", month = \"integer\"))\n",
    "\n",
    "index_cols <- c(\"year\", \"month\")\n",
    "\n",
    "# Clean column names by removing the suffix (like \".SZ\") from stock IDs\n",
    "colnames(stock)[-1] <- gsub(\"\\\\.[A-Z]+$\", \"\", colnames(stock)[-1])\n",
    "\n",
    "# Identify stock return columns (all except date, year, and month)\n",
    "stock_id <- colnames(stock)[!(colnames(stock) %in% index_cols)]\n",
    "\n",
    "# Convert all stock ID columns to numeric\n",
    "for (id in stock_id) {\n",
    "    stock[[id]] <- as.numeric(stock[[id]])\n",
    "}\n",
    "\n",
    "# Display the first few rows of the data\n",
    "head(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbeec2f",
   "metadata": {},
   "source": [
    "- Read listing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a1509",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the list_date.csv file\n",
    "list_time <- read.csv(\"list_time.csv\", header = TRUE, stringsAsFactors = FALSE, colClasses = c(stkcd = \"character\", year = \"integer\", month = \"integer\"))\n",
    "\n",
    "# Clean stock column by removing the suffix (like \".SZ\") from stock IDs\n",
    "list_time$stkcd <- gsub(\"\\\\.[A-Z]+$\", \"\", list_time$stkcd)\n",
    "\n",
    "head(list_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a4ecb",
   "metadata": {},
   "source": [
    "- For each stock, remove observations within the 1 year period after the listing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821677f9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the stock dataframe\n",
    "stock_filtered <- stock\n",
    "\n",
    "# Track statistics\n",
    "affected_stocks <- 0\n",
    "removed_observations <- 0\n",
    "\n",
    "# For each stock, remove observations where it has been listed for less than 1 year\n",
    "for (id in stock_id) {\n",
    "    # Find the stock in list_time\n",
    "    idx <- which(list_time$stkcd == id)\n",
    "    \n",
    "    if (length(idx) > 0) {\n",
    "        # Get the listing year and month\n",
    "        listing_year <- list_time$year[idx]\n",
    "        listing_month <- list_time$month[idx]\n",
    "        \n",
    "        # For each row in stock_filtered\n",
    "        for (row in 1:nrow(stock_filtered)) {\n",
    "            current_year <- stock_filtered$year[row]\n",
    "            current_month <- stock_filtered$month[row]\n",
    "            \n",
    "            # Check if this observation is within 1 year of listing\n",
    "            years_difference <- current_year - listing_year\n",
    "            months_difference <- current_month - listing_month\n",
    "            total_months_difference <- years_difference * 12 + months_difference\n",
    "            \n",
    "            if (total_months_difference < 12) {\n",
    "                # Count if this cell has data (not NA) before setting it to NA\n",
    "                if (!is.na(stock_filtered[row, id])) {\n",
    "                    removed_observations <- removed_observations + 1\n",
    "                    if (sum(!is.na(stock_filtered[, id])) > 0) {\n",
    "                        affected_stocks <- affected_stocks + 1\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Remove this observation\n",
    "                stock_filtered[row, id] <- NA\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Count unique affected stocks (the previous count might double-count)\n",
    "affected_stocks <- sum(sapply(stock_id, function(id) {\n",
    "    any(is.na(stock_filtered[, id])) & any(!is.na(stock[, id]))\n",
    "}))\n",
    "\n",
    "# Print summary\n",
    "cat(\"Removed observations for\", affected_stocks, \"stocks that had been listed for less than 1 year\\n\")\n",
    "cat(\"Total observations removed:\", removed_observations, \"\\n\")\n",
    "\n",
    "# Update the stock dataframe\n",
    "stock <- stock_filtered\n",
    "\n",
    "head(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b3714",
   "metadata": {},
   "source": [
    "- Remove stocks with too many missing values under the prioritized criteria:\n",
    "  1. Remove as fewer stocks as possible\n",
    "  2. Common time scope of the remaining observations should be closer to latest as possible\n",
    "  3. Common time scope of the remaining observations should be as long as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3e357",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the stock dataframe\n",
    "stock_filtered <- stock\n",
    "\n",
    "# Create a matrix indicating non-NA values\n",
    "has_data <- !is.na(stock_filtered[, stock_id])\n",
    "colnames(has_data) <- stock_id\n",
    "\n",
    "# Calculate the percentage of non-NA values for each stock\n",
    "stock_valid_pct <- colMeans(has_data) * 100\n",
    "names(stock_valid_pct) <- stock_id\n",
    "\n",
    "# Create a year-month identifier for easier period tracking\n",
    "stock_filtered$ym <- stock_filtered$year * 100 + stock_filtered$month\n",
    "\n",
    "# Function to find the common non-NA period for a set of stocks\n",
    "find_common_period <- function(stocks_subset) {\n",
    "    # For each period, check if all stocks have data\n",
    "    all_valid <- apply(has_data[, stocks_subset], 1, all)\n",
    "    \n",
    "    # If no common valid dates found, return NULL\n",
    "    if (!any(all_valid)) {\n",
    "        return(NULL)\n",
    "    }\n",
    "    \n",
    "    # Find continuous valid periods\n",
    "    runs <- rle(all_valid)\n",
    "    valid_run_indices <- which(runs$values)\n",
    "    \n",
    "    if (length(valid_run_indices) == 0) {\n",
    "        return(NULL)\n",
    "    }\n",
    "    \n",
    "    # Find all valid runs and their properties\n",
    "    valid_periods <- data.frame(\n",
    "        run_idx = valid_run_indices,\n",
    "        length = runs$lengths[valid_run_indices],\n",
    "        stringsAsFactors = FALSE\n",
    "    )\n",
    "    \n",
    "    # Calculate start and end positions for each run\n",
    "    valid_periods$start_pos <- sapply(valid_periods$run_idx, function(idx) {\n",
    "        if(idx == 1) 1 else sum(runs$lengths[1:(idx-1)]) + 1\n",
    "    })\n",
    "    valid_periods$end_pos <- valid_periods$start_pos + valid_periods$length - 1\n",
    "    \n",
    "    # Add start and end year-month\n",
    "    valid_periods$start_ym <- stock_filtered$ym[valid_periods$start_pos]\n",
    "    valid_periods$end_ym <- stock_filtered$ym[valid_periods$end_pos]\n",
    "    \n",
    "    # Calculate recency score - higher for more recent periods\n",
    "    max_ym <- max(stock_filtered$ym)\n",
    "    valid_periods$recency_score <- valid_periods$end_ym / max_ym\n",
    "    \n",
    "    # Normalize length\n",
    "    max_length <- max(valid_periods$length)\n",
    "    valid_periods$norm_length <- valid_periods$length / max_length\n",
    "    \n",
    "    # Combined score: 60% length, 40% recency (favoring recent periods)\n",
    "    valid_periods$combined_score <- 0.6 * valid_periods$norm_length + 0.4 * valid_periods$recency_score\n",
    "    \n",
    "    # Find the period with the highest combined score\n",
    "    best_idx <- which.max(valid_periods$combined_score)\n",
    "    \n",
    "    return(list(\n",
    "        start_idx = valid_periods$start_pos[best_idx],\n",
    "        end_idx = valid_periods$end_pos[best_idx],\n",
    "        length = valid_periods$length[best_idx],\n",
    "        start_ym = valid_periods$start_ym[best_idx],\n",
    "        end_ym = valid_periods$end_ym[best_idx],\n",
    "        combined_score = valid_periods$combined_score[best_idx]\n",
    "    ))\n",
    "}\n",
    "\n",
    "# Initialize variables to store best solution\n",
    "best_solution <- NULL\n",
    "best_score <- -Inf\n",
    "\n",
    "# Try different thresholds to include more or fewer stocks\n",
    "thresholds <- seq(95, 50, by = -5)\n",
    "\n",
    "for (threshold in thresholds) {\n",
    "    # Select stocks with valid data percentage above threshold\n",
    "    candidate_stocks <- names(stock_valid_pct[stock_valid_pct >= threshold])\n",
    "    \n",
    "    # Skip if too few stocks meet the threshold\n",
    "    if (length(candidate_stocks) < 2) {\n",
    "        next\n",
    "    }\n",
    "    \n",
    "    # Find common period for these stocks\n",
    "    period <- find_common_period(candidate_stocks)\n",
    "    \n",
    "    # Skip if no common period found\n",
    "    if (is.null(period)) {\n",
    "        next\n",
    "    }\n",
    "    \n",
    "    # Total score combines period quality and number of stocks\n",
    "    # Normalize stock count to give more weight to solutions with more stocks\n",
    "    stock_ratio <- length(candidate_stocks) / length(stock_id)\n",
    "    total_score <- period$combined_score + 0.2 * stock_ratio\n",
    "    \n",
    "    # Update best solution if better score found\n",
    "    if (total_score > best_score) {\n",
    "        best_solution <- list(\n",
    "            stocks = candidate_stocks,\n",
    "            start_idx = period$start_idx,\n",
    "            end_idx = period$end_idx,\n",
    "            length = period$length,\n",
    "            start_ym = period$start_ym,\n",
    "            end_ym = period$end_ym\n",
    "        )\n",
    "        best_score <- total_score\n",
    "    }\n",
    "}\n",
    "\n",
    "if (!is.null(best_solution)) {\n",
    "    # Identify stocks to remove\n",
    "    stocks_to_remove <- setdiff(stock_id, best_solution$stocks)\n",
    "    \n",
    "    # Format year-month for display (YYYY-MM)\n",
    "    format_ym <- function(ym) {\n",
    "        year <- floor(ym/100)\n",
    "        month <- ym %% 100\n",
    "        return(paste0(year, \"-\", sprintf(\"%02d\", month)))\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    cat(\"Removing\", length(stocks_to_remove), \"stocks with excessive NA values:\", \n",
    "            paste(stocks_to_remove, collapse=\", \"), \"\\n\")\n",
    "    cat(\"Retained\", length(best_solution$stocks), \"stocks\\n\")\n",
    "    cat(\"Common non-NA period:\", format_ym(best_solution$start_ym), \"to\", \n",
    "            format_ym(best_solution$end_ym), \"(\", best_solution$length, \"periods )\\n\")\n",
    "    \n",
    "    # Update stock_id to the selected stocks\n",
    "    stock_id <- best_solution$stocks\n",
    "    \n",
    "    # Filter the dataframe to keep only the selected stocks and date range\n",
    "    stock_filtered <- stock_filtered[best_solution$start_idx:best_solution$end_idx, c(\"year\", \"month\", best_solution$stocks)]\n",
    "    \n",
    "    # Verify no NA values in the filtered dataset\n",
    "    cat(\"NA values in filtered dataset:\", sum(is.na(stock_filtered)), \"\\n\")\n",
    "} else {\n",
    "    cat(\"Could not find a common non-NA period for any subset of stocks\\n\")\n",
    "}\n",
    "\n",
    "# Update the stock dataframe\n",
    "stock <- stock_filtered\n",
    "\n",
    "head(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca19ca",
   "metadata": {},
   "source": [
    "- Trim extreme values and replace missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d68ba",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the stock dataframe\n",
    "stock_filtered <- stock\n",
    "\n",
    "# Make sure data is ordered by year and month\n",
    "stock_filtered <- stock_filtered[order(stock_filtered$year, stock_filtered$month),]\n",
    "\n",
    "# Initialize counters for reporting\n",
    "total_outliers <- 0\n",
    "total_na_replacements <- 0\n",
    "outliers_by_stock <- numeric(length(stock_id))\n",
    "names(outliers_by_stock) <- stock_id\n",
    "\n",
    "# Function to handle outliers with various methods\n",
    "replace_outliers <- function(x, method = \"interp\") {\n",
    "    # Create a copy of the vector with identified outlier positions as NA\n",
    "    x_replaced <- x\n",
    "    \n",
    "    # Skip if no NA values (no outliers to replace)\n",
    "    if (!any(is.na(x_replaced))) return(x_replaced)\n",
    "    \n",
    "    # Apply chosen replacement method\n",
    "    if (method == \"interp\") {\n",
    "        # Linear interpolation\n",
    "        na_idx <- which(is.na(x_replaced))\n",
    "        for (idx in na_idx) {\n",
    "            # Find nearest non-NA values before and after\n",
    "            before_idx <- idx - 1\n",
    "            while(before_idx > 0 && is.na(x_replaced[before_idx])) before_idx <- before_idx - 1\n",
    "            \n",
    "            after_idx <- idx + 1\n",
    "            while(after_idx <= length(x_replaced) && is.na(x_replaced[after_idx])) after_idx <- after_idx + 1\n",
    "            \n",
    "            # Interpolate if both bounds exist\n",
    "            if (before_idx > 0 && after_idx <= length(x_replaced)) {\n",
    "                x_replaced[idx] <- x_replaced[before_idx] + \n",
    "                    (x_replaced[after_idx] - x_replaced[before_idx]) * \n",
    "                    (idx - before_idx) / (after_idx - before_idx)\n",
    "            } else if (before_idx > 0) {\n",
    "                # If only before exists, use that value\n",
    "                x_replaced[idx] <- x_replaced[before_idx]\n",
    "            } else if (after_idx <= length(x_replaced)) {\n",
    "                # If only after exists, use that value\n",
    "                x_replaced[idx] <- x_replaced[after_idx]\n",
    "            } else {\n",
    "                # Fallback to zero if no reference points\n",
    "                x_replaced[idx] <- 0\n",
    "            }\n",
    "        }\n",
    "    } else if (method == \"median\") {\n",
    "        # Replace with rolling median (window of 5 months)\n",
    "        window_size <- 5\n",
    "        na_idx <- which(is.na(x_replaced))\n",
    "        for (idx in na_idx) {\n",
    "            # Define window bounds\n",
    "            start <- max(1, idx - floor(window_size/2))\n",
    "            end <- min(length(x), idx + floor(window_size/2))\n",
    "            \n",
    "            # Get values in window excluding the current NA\n",
    "            window_values <- x[start:end]\n",
    "            window_values <- window_values[!is.na(window_values)]\n",
    "            \n",
    "            if (length(window_values) > 0) {\n",
    "                x_replaced[idx] <- median(window_values)\n",
    "            } else {\n",
    "                x_replaced[idx] <- 0 # Fallback if all window values are NA\n",
    "            }\n",
    "        }\n",
    "    } else if (method == \"zero\") {\n",
    "        # Simple zero replacement\n",
    "        x_replaced[is.na(x_replaced)] <- 0\n",
    "    }\n",
    "    \n",
    "    return(x_replaced)\n",
    "}\n",
    "\n",
    "# Clean each return series\n",
    "for (id in stock_id) {\n",
    "    # Use rolling window for adaptive outlier detection\n",
    "    window_size <- 12 # 12-month rolling window (adjusted from 20 days)\n",
    "    n <- length(stock_filtered[,id])\n",
    "    \n",
    "    for (i in window_size:n) {\n",
    "        # Define the rolling window\n",
    "        window_start <- i - window_size + 1\n",
    "        window_data <- stock_filtered[window_start:i, id]\n",
    "        current_value <- window_data[window_size]\n",
    "        \n",
    "        # Skip if the current value is NA\n",
    "        if (is.na(current_value)) next\n",
    "        \n",
    "        # Calculate robust statistics from the window (excluding current point)\n",
    "        window_prev <- window_data[-window_size]\n",
    "        window_median <- median(window_prev, na.rm = TRUE)\n",
    "        window_mad <- mad(window_prev, na.rm = TRUE)\n",
    "        \n",
    "        # Skip if median or MAD is NA or MAD is 0 (no variability)\n",
    "        if (is.na(window_median) || is.na(window_mad) || window_mad == 0) next\n",
    "        \n",
    "        # Use median absolute deviation (MAD) for robust outlier detection\n",
    "        mad_threshold <- 5 # 5 MADs is a common threshold for extreme outliers\n",
    "        \n",
    "        # Check if the current value is an outlier\n",
    "        if (abs(current_value - window_median) > mad_threshold * window_mad) {\n",
    "            # Mark as outlier\n",
    "            stock_filtered[i, id] <- NA\n",
    "            outliers_by_stock[id] <- outliers_by_stock[id] + 1\n",
    "            total_outliers <- total_outliers + 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Replace outliers and NAs with chosen method\n",
    "    if (any(is.na(stock_filtered[,id]))) {\n",
    "        na_count_before <- sum(is.na(stock_filtered[,id]))\n",
    "        stock_filtered[,id] <- replace_outliers(stock_filtered[,id], method = \"interp\")\n",
    "        na_count_after <- sum(is.na(stock_filtered[,id]))\n",
    "        total_na_replacements <- total_na_replacements + (na_count_before - na_count_after)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print report\n",
    "cat(\"Extreme Value Removal Report:\\n\")\n",
    "cat(\"----------------------------\\n\")\n",
    "cat(\"Total outliers identified and removed:\", total_outliers, \"\\n\")\n",
    "cat(\"Total missing values replaced:\", total_na_replacements, \"\\n\\n\")\n",
    "\n",
    "# Print stocks with the most outliers\n",
    "if (total_outliers > 0) {\n",
    "    cat(\"Stocks with outliers:\\n\")\n",
    "    for (id in stock_id) {\n",
    "        if (outliers_by_stock[id] > 0) {\n",
    "            cat(\"  -\", id, \":\", outliers_by_stock[id], \"outliers\\n\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use the cleaned data for further analysis\n",
    "stock <- stock_filtered\n",
    "\n",
    "head(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf45e74",
   "metadata": {},
   "source": [
    "- Detrend the return series using HP filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8505ab",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(mFilter)\n",
    "\n",
    "# Create a new dataframe to store the cyclical components with year and month\n",
    "cycle <- data.frame(year = stock$year, month = stock$month)\n",
    "\n",
    "# Ensure the data is sorted chronologically\n",
    "cycle <- cycle[order(cycle$year, cycle$month), ]\n",
    "\n",
    "# Apply HP filter to each stock return series and extract the cyclical component\n",
    "for (id in stock_id) {\n",
    "    # Apply HP filter with lambda=14400 (commonly used for monthly financial data)\n",
    "    hp_filter <- hpfilter(stock[,id], freq=14400)\n",
    "    \n",
    "    # Extract the cyclical component and add to cycle dataframe\n",
    "    cycle[,id] <- hp_filter$cycle\n",
    "}\n",
    "\n",
    "# Display the first few rows of the cycle dataframe\n",
    "head(cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770b844",
   "metadata": {},
   "source": [
    "### VIX\n",
    "\n",
    "> Daily, close value of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee71ab4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the VIX data using read.csv\n",
    "vix <- read.csv(\"vix.csv\", header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)\n",
    "\n",
    "# Convert date column to Date format using base R syntax\n",
    "vix$date <- as.Date(vix$date, format = \"%m/%d/%Y\")\n",
    "\n",
    "# Filter to keep only date and close columns\n",
    "vix <- vix[, c(\"date\", \"close\")]\n",
    "\n",
    "# Sort data by date to ensure chronological order\n",
    "vix <- vix[order(vix$date), ]\n",
    "\n",
    "# Identify gaps in the time series\n",
    "date_range <- range(vix$date)\n",
    "complete_dates <- seq(date_range[1], date_range[2], by = \"day\")\n",
    "missing_dates <- complete_dates[!complete_dates %in% vix$date]\n",
    "\n",
    "# Report missing dates\n",
    "cat(\"Found\", length(missing_dates), \"missing dates in VIX time series\\n\")\n",
    "\n",
    "# Create a complete dataframe with all dates\n",
    "complete_vix <- data.frame(date = complete_dates)\n",
    "\n",
    "# Merge with actual data, which will result in NA for missing dates\n",
    "vix_filled <- merge(complete_vix, vix, by = \"date\", all.x = TRUE)\n",
    "\n",
    "# Fill missing values using linear interpolation\n",
    "library(zoo)\n",
    "vix_filled$close <- na.approx(vix_filled$close, na.rm = FALSE)\n",
    "\n",
    "# Handle any remaining NAs at the beginning or end \n",
    "# that couldn't be interpolated\n",
    "if (any(is.na(vix_filled$close))) {\n",
    "    # Fill NAs at beginning with next observation\n",
    "    vix_filled$close <- na.locf(vix_filled$close, fromLast = TRUE, na.rm = FALSE)\n",
    "    \n",
    "    # Fill NAs at end with last observation\n",
    "    vix_filled$close <- na.locf(vix_filled$close, na.rm = FALSE)\n",
    "    \n",
    "    cat(\"Filled remaining edge values using last/next observation carrying\\n\")\n",
    "}\n",
    "\n",
    "# Update the vix dataframe\n",
    "vix <- vix_filled\n",
    "\n",
    "# Quick summary\n",
    "cat(\"VIX data now has\", nrow(vix), \"daily observations with no gaps\\n\")\n",
    "\n",
    "# Display the first few rows of the filtered VIX data\n",
    "head(vix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0519082",
   "metadata": {},
   "source": [
    "### GFCF\n",
    "\n",
    "> Monthly, Global Financial Cycle Factor, Common factor across world risky asset prices as in Miranda-Agrippino and Rey (2020), *\"US Monetary Policy and the Global Financial Cycle\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7554c5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the GFCF data\n",
    "gfcf <- read.csv(\"gfcf.csv\", header = TRUE, stringsAsFactors = FALSE, colClasses = c(year = \"integer\", month = \"integer\"))\n",
    "\n",
    "# Sort by date to ensure chronological order\n",
    "gfcf <- gfcf[order(gfcf$year, gfcf$month), ]\n",
    "\n",
    "# Check for gaps in year-month sequence\n",
    "min_year <- min(gfcf$year)\n",
    "max_year <- max(gfcf$year)\n",
    "min_month <- min(gfcf$month[gfcf$year == min_year])\n",
    "max_month <- max(gfcf$month[gfcf$year == max_year])\n",
    "\n",
    "# Create complete sequence of year-month combinations\n",
    "all_dates <- expand.grid(\n",
    "    year = min_year:max_year,\n",
    "    month = 1:12\n",
    ")\n",
    "all_dates <- all_dates[\n",
    "    (all_dates$year > min_year | all_dates$month >= min_month) &\n",
    "    (all_dates$year < max_year | all_dates$month <= max_month),\n",
    "]\n",
    "all_dates <- all_dates[order(all_dates$year, all_dates$month), ]\n",
    "\n",
    "# Merge with actual data to identify gaps\n",
    "complete_gfcf <- merge(all_dates, gfcf, by = c(\"year\", \"month\"), all.x = TRUE)\n",
    "\n",
    "# Check for missing values after merge\n",
    "missing_count <- sum(is.na(complete_gfcf$gfcf))\n",
    "\n",
    "if (missing_count > 0) {\n",
    "    cat(\"Found\", missing_count, \"missing year-month combinations in the GFCF time series\\n\")\n",
    "    \n",
    "    # Fill missing values using interpolation\n",
    "    library(zoo)\n",
    "    \n",
    "    # Create a time index for interpolation (decimal year)\n",
    "    complete_gfcf$time_idx <- complete_gfcf$year + (complete_gfcf$month - 1) / 12\n",
    "    \n",
    "    # Interpolate missing values\n",
    "    complete_gfcf$gfcf <- na.approx(complete_gfcf$gfcf, x = complete_gfcf$time_idx, na.rm = FALSE)\n",
    "    \n",
    "    # Fill any remaining NAs at the beginning or end\n",
    "    complete_gfcf$gfcf <- na.locf(complete_gfcf$gfcf, fromLast = TRUE, na.rm = FALSE)\n",
    "    complete_gfcf$gfcf <- na.locf(complete_gfcf$gfcf, na.rm = FALSE)\n",
    "    \n",
    "    # Remove the time index column (was just for interpolation)\n",
    "    complete_gfcf$time_idx <- NULL\n",
    "} else {\n",
    "    cat(\"No gaps detected in the GFCF year-month sequence\\n\")\n",
    "}\n",
    "\n",
    "# Update the gfcf dataframe\n",
    "gfcf <- complete_gfcf[order(complete_gfcf$year, complete_gfcf$month), ]\n",
    "\n",
    "# Verify no more missing values\n",
    "final_missing <- sum(is.na(gfcf$gfcf))\n",
    "cat(\"Missing values after filling:\", final_missing, \"\\n\")\n",
    "\n",
    "# Display summary statistics\n",
    "cat(\"\\nGFCF data summary:\\n\")\n",
    "summary(gfcf$gfcf)\n",
    "\n",
    "# Display the first few rows\n",
    "head(gfcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62919ff4",
   "metadata": {},
   "source": [
    "### Capital Inflow\n",
    "\n",
    "> Quarterly, normalized by quarter GDP value in USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef4f9a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the capital inflow data\n",
    "inflow <- read.csv(\"channel_inflow.csv\", header = TRUE, stringsAsFactors = FALSE, \n",
    "                   colClasses = c(year = \"integer\", quarter = \"integer\"))\n",
    "\n",
    "# Sort by date to ensure chronological order\n",
    "inflow <- inflow[order(inflow$year, inflow$quarter), ]\n",
    "\n",
    "# Check for gaps in year-quarter sequence\n",
    "min_year <- min(inflow$year)\n",
    "max_year <- max(inflow$year)\n",
    "min_quarter <- min(inflow$quarter[inflow$year == min_year])\n",
    "max_quarter <- max(inflow$quarter[inflow$year == max_year])\n",
    "\n",
    "# Create complete sequence of year-quarter combinations\n",
    "all_dates <- expand.grid(\n",
    "    year = min_year:max_year,\n",
    "    quarter = 1:4\n",
    ")\n",
    "all_dates <- all_dates[\n",
    "    (all_dates$year > min_year | all_dates$quarter >= min_quarter) &\n",
    "    (all_dates$year < max_year | all_dates$quarter <= max_quarter),\n",
    "]\n",
    "all_dates <- all_dates[order(all_dates$year, all_dates$quarter), ]\n",
    "\n",
    "# Add month column for each quarter's end (Q1=3, Q2=6, Q3=9, Q4=12)\n",
    "all_dates$month <- all_dates$quarter * 3\n",
    "\n",
    "# Merge with actual data to identify gaps\n",
    "complete_inflow <- merge(all_dates, inflow, by = c(\"year\", \"quarter\"), all.x = TRUE)\n",
    "\n",
    "# Check for missing values after merge\n",
    "missing_count <- sum(is.na(complete_inflow$inflow))\n",
    "\n",
    "if (missing_count > 0) {\n",
    "    cat(\"Found\", missing_count, \"missing year-quarter combinations in the inflow time series\\n\")\n",
    "    \n",
    "    # Fill missing values using interpolation\n",
    "    library(zoo)\n",
    "    \n",
    "    # Create a time index for interpolation (decimal year)\n",
    "    complete_inflow$time_idx <- complete_inflow$year + (complete_inflow$quarter - 1) / 4\n",
    "    \n",
    "    # Interpolate missing values\n",
    "    complete_inflow$inflow <- na.approx(complete_inflow$inflow, x = complete_inflow$time_idx, na.rm = FALSE)\n",
    "    \n",
    "    # Fill any remaining NAs at the beginning or end\n",
    "    complete_inflow$inflow <- na.locf(complete_inflow$inflow, fromLast = TRUE, na.rm = FALSE)\n",
    "    complete_inflow$inflow <- na.locf(complete_inflow$inflow, na.rm = FALSE)\n",
    "    \n",
    "    # Remove the time index column (was just for interpolation)\n",
    "    complete_inflow$time_idx <- NULL\n",
    "} else {\n",
    "    cat(\"No gaps detected in the inflow year-quarter sequence\\n\")\n",
    "}\n",
    "\n",
    "# Update the inflow dataframe\n",
    "inflow <- complete_inflow[order(complete_inflow$year, complete_inflow$quarter), ]\n",
    "\n",
    "# Verify no more missing values\n",
    "final_missing <- sum(is.na(inflow$inflow))\n",
    "cat(\"Missing values after filling:\", final_missing, \"\\n\")\n",
    "\n",
    "# Display summary statistics\n",
    "cat(\"\\nCapital inflow data summary:\\n\")\n",
    "summary(inflow$inflow)\n",
    "\n",
    "# Display the first few rows\n",
    "head(inflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317b3fa",
   "metadata": {},
   "source": [
    "### Economic Policy Uncertainty (EPU)\n",
    "\n",
    "> Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec3b0c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the EPU data\n",
    "epu <- read.csv(\"channel_epu.csv\", header = TRUE, stringsAsFactors = FALSE, \n",
    "               colClasses = c(year = \"integer\", month = \"integer\"))\n",
    "\n",
    "# Add quarter column\n",
    "epu$quarter <- as.integer(ceiling(epu$month / 3))\n",
    "\n",
    "# Sort by date to ensure chronological order\n",
    "epu <- epu[order(epu$year, epu$quarter, epu$month), ]\n",
    "\n",
    "# Check for missing values in epu\n",
    "cat(\"Missing values in EPU:\", sum(is.na(epu$epu)), \"\\n\")\n",
    "\n",
    "# Handle any missing values if necessary\n",
    "if(sum(is.na(epu$epu)) > 0) {\n",
    "  # Use linear interpolation for missing values\n",
    "  library(zoo)\n",
    "  epu$epu <- na.approx(epu$epu, na.rm = FALSE)\n",
    "  # Handle any remaining edge cases\n",
    "  epu$epu <- na.locf(epu$epu, fromLast = TRUE, na.rm = FALSE)\n",
    "  epu$epu <- na.locf(epu$epu, na.rm = FALSE)\n",
    "}\n",
    "\n",
    "# Calculate quarterly average EPU values\n",
    "quarterly_epu <- aggregate(epu ~ year + quarter, data = epu, FUN = mean)\n",
    "\n",
    "# Add month column (last month of each quarter)\n",
    "quarterly_epu$month <- quarterly_epu$quarter * 3\n",
    "\n",
    "# Calculate log of quarterly average EPU\n",
    "quarterly_epu$log_epu <- log(quarterly_epu$epu)\n",
    "\n",
    "# Select only the needed columns\n",
    "epu <- quarterly_epu[, c(\"year\", \"quarter\", \"month\", \"log_epu\")]\n",
    "\n",
    "# Sort by year, quarter\n",
    "epu <- epu[order(epu$year, epu$quarter), ]\n",
    "\n",
    "# Display summary\n",
    "cat(\"EPU data processed with\", nrow(epu), \"quarterly observations\\n\")\n",
    "summary(epu$log_epu)\n",
    "\n",
    "# Display the first few rows\n",
    "head(epu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff0b40",
   "metadata": {},
   "source": [
    "### Control\n",
    "\n",
    "> Quarterly, quarter-end month observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd7de2",
   "metadata": {},
   "source": [
    "#### Micro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555de57",
   "metadata": {},
   "source": [
    "- Read the micro part of control variable and balance the panel\n",
    "\n",
    "- Dataset has already received the following cleansing before input:\n",
    "  1. Removed:\n",
    "     - stocks from the financial sector\n",
    "     - stocks incurred ST/ST*/PT during observation period\n",
    "  2. Winsorized: 5% on both ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c26bd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the control_micro data\n",
    "control_micro <- read.csv(\"control_micro.csv\", header = TRUE, stringsAsFactors = FALSE, \n",
    "        colClasses = c(stkcd = \"character\", year = \"integer\", quarter = \"integer\", month = \"integer\"))\n",
    "\n",
    "index_cols <- c(\"stkcd\", \"year\", \"quarter\", \"month\")\n",
    "var_cols <- c(\"Size\", \"Lev\", \"ROA\", \"BM\", \"Board\", \"Top1\", \"SOE\")\n",
    "\n",
    "control_micro <- control_micro[, c(index_cols, var_cols)]\n",
    "\n",
    "# Find the earliest and latest year-quarter combinations\n",
    "min_year <- min(control_micro$year)\n",
    "min_quarter <- min(control_micro$quarter[control_micro$year == min_year])\n",
    "max_year <- max(control_micro$year)\n",
    "max_quarter <- max(control_micro$quarter[control_micro$year == max_year])\n",
    "\n",
    "cat(\"Date range for all stocks:\", min_year, \"Q\", min_quarter, \"to\", max_year, \"Q\", max_quarter, \"\\n\")\n",
    "\n",
    "# Create a complete sequence of year-quarter combinations\n",
    "all_dates <- expand.grid(\n",
    "  year = min_year:max_year,\n",
    "  quarter = 1:4\n",
    ")\n",
    "\n",
    "# Filter to keep only dates within the identified range\n",
    "all_dates <- all_dates[\n",
    "  (all_dates$year > min_year | all_dates$quarter >= min_quarter) &\n",
    "  (all_dates$year < max_year | all_dates$quarter <= max_quarter),\n",
    "]\n",
    "all_dates <- all_dates[order(all_dates$year, all_dates$quarter), ]\n",
    "\n",
    "# Add month column based on quarter (Q1=3, Q2=6, Q3=9, Q4=12)\n",
    "all_dates$month <- all_dates$quarter * 3\n",
    "\n",
    "# Get unique stock codes\n",
    "unique_stocks <- unique(control_micro$stkcd)\n",
    "\n",
    "# Create an empty dataframe to hold the balanced panel\n",
    "balanced_control_micro <- data.frame()\n",
    "\n",
    "# For each stock, add missing dates\n",
    "for (stock in unique_stocks) {\n",
    "  # Get data for this stock\n",
    "  stock_data <- control_micro[control_micro$stkcd == stock, ]\n",
    "  \n",
    "  # Create stock-specific complete date sequence\n",
    "  stock_dates <- data.frame(\n",
    "  stkcd = stock,\n",
    "  year = all_dates$year,\n",
    "  quarter = all_dates$quarter,\n",
    "  month = all_dates$month\n",
    "  )\n",
    "  \n",
    "  # Merge with actual data (will add NA for missing dates)\n",
    "  complete_stock_data <- merge(\n",
    "  stock_dates,\n",
    "  stock_data,\n",
    "  by = index_cols,\n",
    "  all.x = TRUE\n",
    "  )\n",
    "  \n",
    "  # Add to the balanced panel\n",
    "  balanced_control_micro <- rbind(balanced_control_micro, complete_stock_data)\n",
    "}\n",
    "\n",
    "# Sort the balanced panel\n",
    "balanced_control_micro <- balanced_control_micro[order(balanced_control_micro$stkcd, balanced_control_micro$year, balanced_control_micro$quarter), c(index_cols, var_cols)]\n",
    "\n",
    "# Display summary statistics\n",
    "cat(\"Original data had\", nrow(control_micro), \"observations\\n\")\n",
    "cat(\"Balanced panel has\", nrow(balanced_control_micro), \"observations\\n\")\n",
    "cat(\"Added\", nrow(balanced_control_micro) - nrow(control_micro), \"missing observations\\n\")\n",
    "\n",
    "# Calculate how many stocks have missing observations\n",
    "stocks_with_gaps <- sapply(unique_stocks, function(stock) {\n",
    "  stock_data <- control_micro[control_micro$stkcd == stock, ]\n",
    "  expected_obs <- nrow(all_dates)\n",
    "  actual_obs <- nrow(stock_data)\n",
    "  return(expected_obs != actual_obs)\n",
    "})\n",
    "\n",
    "cat(\"Number of stocks with time gaps:\", sum(stocks_with_gaps), \"out of\", length(unique_stocks), \"\\n\")\n",
    "\n",
    "# Update the control_micro dataframe\n",
    "control_micro <- balanced_control_micro\n",
    "\n",
    "# Display the first few rows\n",
    "head(control_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2d16a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read industry classification data\n",
    "ind_type <- read.csv(\"ind_type.csv\", header = TRUE, stringsAsFactors = FALSE, \n",
    "                    colClasses = c(stkcd = \"character\", year = \"integer\"))\n",
    "\n",
    "# Ensure stock code format consistency for merging\n",
    "if(any(grepl(\"\\\\.\", ind_type$stkcd))) {\n",
    "  ind_type$stkcd <- gsub(\"\\\\.[A-Z]+$\", \"\", ind_type$stkcd)\n",
    "}\n",
    "\n",
    "# Check for duplicate entries\n",
    "duplicates <- ind_type[duplicated(ind_type[, c(\"year\", \"stkcd\")]) | \n",
    "                      duplicated(ind_type[, c(\"year\", \"stkcd\")], fromLast = TRUE), ]\n",
    "if(nrow(duplicates) > 0) {\n",
    "  cat(\"Warning: Found\", nrow(duplicates)/2, \"duplicate year-stock combinations in industry data\\n\")\n",
    "}\n",
    "\n",
    "# Merge with control_micro using inner join\n",
    "control_micro_with_ind <- merge(\n",
    "  control_micro,\n",
    "  ind_type,\n",
    "  by = c(\"year\", \"stkcd\"),\n",
    "  all.x = FALSE\n",
    ")\n",
    "\n",
    "# Check dimensions before and after merge\n",
    "cat(\"Rows in control_micro before merge:\", nrow(control_micro), \"\\n\")\n",
    "cat(\"Rows after merge with industry data:\", nrow(control_micro_with_ind), \"\\n\")\n",
    "cat(\"Stocks with missing industry information:\", \n",
    "    length(setdiff(unique(control_micro$stkcd), unique(control_micro_with_ind$stkcd))), \"\\n\")\n",
    "\n",
    "# Convert industry codes to factors for regression analysis\n",
    "control_micro_with_ind$ind <- factor(control_micro_with_ind$ind)\n",
    "\n",
    "# Update the control_micro dataframe\n",
    "control_micro <- control_micro_with_ind\n",
    "\n",
    "# Display the first few rows of the merged data\n",
    "head(control_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ce926",
   "metadata": {},
   "source": [
    "- Treat the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32310c79",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(zoo)\n",
    "\n",
    "# Check how many missing values we have in each column\n",
    "missing_values <- colSums(is.na(control_micro))\n",
    "cat(\"Missing values per column:\\n\")\n",
    "print(missing_values)\n",
    "\n",
    "# Calculate percentage of missing values by column\n",
    "pct_missing <- round(missing_values / nrow(control_micro) * 100, 2)\n",
    "cat(\"\\nPercentage of missing values:\\n\")\n",
    "print(pct_missing)\n",
    "\n",
    "# For visualization, get missing pattern by stock and time\n",
    "missing_by_stock <- tapply(rowSums(is.na(control_micro[, var_cols])), \n",
    "                          control_micro$stkcd, mean)\n",
    "missing_by_time <- aggregate(rowSums(is.na(control_micro[, var_cols])), \n",
    "                            by = list(year = control_micro$year, \n",
    "                                     quarter = control_micro$quarter), \n",
    "                            mean)\n",
    "\n",
    "# Impute missing values based on variable characteristics\n",
    "control_micro_imputed <- control_micro\n",
    "\n",
    "# Approach for each variable:\n",
    "for (id in unique(control_micro$stkcd)) {\n",
    "  # Get data for this stock only\n",
    "  stock_data <- control_micro[control_micro$stkcd == id, ]\n",
    "  stock_data <- stock_data[order(stock_data$year, stock_data$quarter), ]\n",
    "  \n",
    "  for (var in var_cols) {\n",
    "    # Skip if no missing values for this variable and stock\n",
    "    if (!any(is.na(stock_data[, var]))) next\n",
    "    \n",
    "    # Choose imputation method based on variable type\n",
    "    if (var %in% c(\"Size\", \"Lev\", \"ROA\", \"BM\")) {\n",
    "      # For financial metrics that change gradually, use interpolation\n",
    "      stock_data[, var] <- na.approx(stock_data[, var], na.rm = FALSE)\n",
    "      \n",
    "      # Handle any remaining NAs at the beginning with next observation\n",
    "      if (any(is.na(stock_data[, var]))) {\n",
    "        stock_data[, var] <- na.locf(stock_data[, var], fromLast = TRUE, na.rm = FALSE)\n",
    "        \n",
    "        # Handle any remaining NAs at the end with last observation\n",
    "        stock_data[, var] <- na.locf(stock_data[, var], na.rm = FALSE)\n",
    "      }\n",
    "    } else if (var %in% c(\"Board\", \"Top1\", \"SOE\", \"ind1\", \"ind2\")) {\n",
    "      # For governance metrics that change infrequently, use LOCF\n",
    "      stock_data[, var] <- na.locf(stock_data[, var], na.rm = FALSE)\n",
    "      \n",
    "      # If there are still NAs at the beginning, use NOCB\n",
    "      if (any(is.na(stock_data[, var]))) {\n",
    "        stock_data[, var] <- na.locf(stock_data[, var], fromLast = TRUE, na.rm = FALSE)\n",
    "      }\n",
    "      \n",
    "      # For any remaining NAs (if entire series is NA), use cross-sectional median\n",
    "      if (any(is.na(stock_data[, var]))) {\n",
    "        # Get the median value from the whole dataset for this variable\n",
    "        var_median <- median(control_micro[, var], na.rm = TRUE)\n",
    "        stock_data[is.na(stock_data[, var]), var] <- var_median\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Update the imputed dataset with this stock's data\n",
    "  control_micro_imputed[control_micro_imputed$stkcd == id, ] <- stock_data\n",
    "}\n",
    "\n",
    "# Check if any missing values remain\n",
    "missing_after <- colSums(is.na(control_micro_imputed))\n",
    "cat(\"\\nMissing values after imputation:\\n\")\n",
    "print(missing_after)\n",
    "\n",
    "# Display summary statistics before and after imputation\n",
    "cat(\"\\nSummary before imputation:\\n\")\n",
    "print(summary(control_micro[, var_cols]))\n",
    "\n",
    "cat(\"\\nSummary after imputation:\\n\")\n",
    "print(summary(control_micro_imputed[, var_cols]))\n",
    "\n",
    "# Update the control_micro dataframe\n",
    "control_micro <- control_micro_imputed\n",
    "\n",
    "# Display the first few rows of the imputed data\n",
    "head(control_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e48a63",
   "metadata": {},
   "source": [
    "#### Macro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a87f4",
   "metadata": {},
   "source": [
    "- Read the macro part of control variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81f8af",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the control_macro data\n",
    "control_macro <- read.csv(\"control_macro.csv\", header = TRUE, stringsAsFactors = FALSE, \n",
    "              colClasses = c(year = \"integer\", quarter = \"integer\", month = \"integer\"))\n",
    "\n",
    "index_cols <- c(\"year\", \"quarter\", \"month\")\n",
    "var_cols <- c(\"gdp_growth_rate\", \"m2_growth_rate\")\n",
    "\n",
    "# Sort by date to ensure chronological order\n",
    "control_macro <- control_macro[order(control_macro$year, control_macro$quarter), ][, c(index_cols, var_cols)]\n",
    "\n",
    "# Find the earliest and latest year-quarter combinations\n",
    "min_year <- min(control_macro$year)\n",
    "min_quarter <- min(control_macro$quarter[control_macro$year == min_year])\n",
    "max_year <- max(control_macro$year)\n",
    "max_quarter <- max(control_macro$quarter[control_macro$year == max_year])\n",
    "\n",
    "cat(\"Date range:\", min_year, \"Q\", min_quarter, \"to\", max_year, \"Q\", max_quarter, \"\\n\")\n",
    "\n",
    "# Create a complete sequence of year-quarter combinations\n",
    "all_dates <- expand.grid(\n",
    "  year = min_year:max_year,\n",
    "  quarter = 1:4\n",
    ")\n",
    "\n",
    "# Filter to keep only dates within the identified range\n",
    "all_dates <- all_dates[\n",
    "  (all_dates$year > min_year | all_dates$quarter >= min_quarter) &\n",
    "  (all_dates$year < max_year | all_dates$quarter <= max_quarter),\n",
    "]\n",
    "all_dates <- all_dates[order(all_dates$year, all_dates$quarter), ]\n",
    "\n",
    "# Add month column (last month of each quarter)\n",
    "all_dates$month <- all_dates$quarter * 3\n",
    "\n",
    "# Merge with actual data (will add NA for missing dates)\n",
    "complete_control_macro <- merge(\n",
    "  all_dates,\n",
    "  control_macro,\n",
    "  by = index_cols,\n",
    "  all.x = TRUE\n",
    ")\n",
    "\n",
    "# Handle merged month columns if they exist\n",
    "if (\"month.y\" %in% colnames(complete_control_macro)) {\n",
    "  # Use month.y values where available, otherwise use month.x\n",
    "  complete_control_macro$month <- ifelse(\n",
    "    is.na(complete_control_macro$month.y),\n",
    "    complete_control_macro$month.x,\n",
    "    complete_control_macro$month.y\n",
    "  )\n",
    "  \n",
    "  # Remove the extra month columns\n",
    "  complete_control_macro$month.x <- NULL\n",
    "  complete_control_macro$month.y <- NULL\n",
    "}\n",
    "\n",
    "# Count missing periods\n",
    "non_date_cols <- setdiff(names(complete_control_macro), index_cols)\n",
    "missing_periods <- sum(apply(complete_control_macro[, non_date_cols, drop=FALSE], 1, function(x) all(is.na(x))))\n",
    "\n",
    "# Report results\n",
    "cat(\"Found\", missing_periods, \"missing year-quarter combinations\\n\")\n",
    "if (missing_periods > 0) {\n",
    "  cat(\"Added missing periods with NA values for observations\\n\")\n",
    "}\n",
    "\n",
    "# Update the control_macro dataframe\n",
    "control_macro <- complete_control_macro[order(complete_control_macro$year, complete_control_macro$quarter), ]\n",
    "\n",
    "# Display the first few rows\n",
    "head(control_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85060f5a",
   "metadata": {},
   "source": [
    "- Treat the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884d79f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check current missing values in control_macro data\n",
    "missing_values <- colSums(is.na(control_macro))\n",
    "cat(\"Missing values in control_macro:\\n\")\n",
    "print(missing_values)\n",
    "\n",
    "# Impute missing values for each economic variable\n",
    "library(zoo)\n",
    "library(forecast)\n",
    "\n",
    "# Create a time index for better time series handling\n",
    "control_macro$time_idx <- control_macro$year + (control_macro$quarter - 1) / 4\n",
    "\n",
    "# Sort data by time\n",
    "control_macro <- control_macro[order(control_macro$time_idx), ]\n",
    "\n",
    "# Function to impute missing economic time series data\n",
    "impute_economic_ts <- function(x, time_idx) {\n",
    "    # If no missing values, return as is\n",
    "    if(!any(is.na(x))) return(x)\n",
    "    \n",
    "    # For short gaps (1-2 periods), use spline interpolation\n",
    "    x_spline <- na.spline(x, na.rm = FALSE)\n",
    "    \n",
    "    # For remaining gaps, try time series methods\n",
    "    if(any(is.na(x_spline))) {\n",
    "        # Create a time series object\n",
    "        ts_data <- ts(x, frequency = 4)  # Quarterly data has frequency 4\n",
    "        \n",
    "        # Simple imputation with seasonal components\n",
    "        x_seadec <- na.seadec(ts_data, algorithm = \"interpolation\")\n",
    "        \n",
    "        # Replace values still missing after spline with these estimates\n",
    "        na_idx <- which(is.na(x_spline))\n",
    "        x_spline[na_idx] <- x_seadec[na_idx]\n",
    "        \n",
    "        # If still have missing values (e.g., at edges), use ARIMA forecasting\n",
    "        if(any(is.na(x_spline))) {\n",
    "            # First fill leading NAs with the first non-NA value\n",
    "            if(is.na(x_spline[1])) {\n",
    "                first_valid <- min(which(!is.na(x_spline)))\n",
    "                x_spline[1:first_valid-1] <- x_spline[first_valid]\n",
    "            }\n",
    "            \n",
    "            # Then fill trailing NAs with the last non-NA value\n",
    "            if(is.na(x_spline[length(x_spline)])) {\n",
    "                last_valid <- max(which(!is.na(x_spline)))\n",
    "                x_spline[last_valid+1:length(x_spline)] <- x_spline[last_valid]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return(x_spline)\n",
    "}\n",
    "\n",
    "# Apply imputation to each economic variable\n",
    "control_macro$gdp_growth_rate <- impute_economic_ts(control_macro$gdp_growth_rate, control_macro$time_idx)\n",
    "control_macro$m2_growth_rate <- impute_economic_ts(control_macro$m2_growth_rate, control_macro$time_idx)\n",
    "\n",
    "# Remove the time index after imputation\n",
    "control_macro$time_idx <- NULL\n",
    "\n",
    "# Check if imputation was successful\n",
    "missing_after <- colSums(is.na(control_macro))\n",
    "cat(\"\\nMissing values after imputation:\\n\")\n",
    "print(missing_after)\n",
    "\n",
    "# Verify the imputed data makes economic sense\n",
    "cat(\"\\nSummary of imputed economic variables:\\n\")\n",
    "cat(\"GDP growth rate summary:\\n\")\n",
    "print(summary(control_macro$gdp_growth_rate))\n",
    "cat(\"\\nM2 growth rate summary:\\n\")\n",
    "print(summary(control_macro$m2_growth_rate))\n",
    "\n",
    "# Display the first few rows to check for patterns\n",
    "head(control_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f67d3",
   "metadata": {},
   "source": [
    "#### Merge Micro & Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b00b7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Merge the control_micro and control_macro datasets\n",
    "# This will add macro variables to each stock in each time period\n",
    "\n",
    "# First, check dimensions of input datasets\n",
    "cat(\"Control_micro dimensions:\", dim(control_micro)[1], \"observations,\", dim(control_micro)[2], \"variables\\n\")\n",
    "cat(\"Control_macro dimensions:\", dim(control_macro)[1], \"observations,\", dim(control_macro)[2], \"variables\\n\")\n",
    "\n",
    "# Perform the merge using merge() function with an inner join approach\n",
    "control <- merge(\n",
    "    control_micro,  # Stock-level micro controls\n",
    "    control_macro,  # Time-level macro controls\n",
    "    by = c(\"year\", \"quarter\", \"month\"),  # Join keys\n",
    "    all.x = FALSE,  # Inner join to keep only matched records\n",
    "    all.y = FALSE\n",
    ")\n",
    "\n",
    "# Verify the structure of the merged dataset\n",
    "cat(\"\\nMerged control dataset dimensions:\", dim(control)[1], \"observations,\", dim(control)[2], \"variables\\n\")\n",
    "cat(\"Number of stocks in merged dataset:\", length(unique(control$stkcd)), \"\\n\")\n",
    "cat(\"Time period covered:\", min(paste(control$year, \"Q\", control$quarter)), \"to\", \n",
    "        max(paste(control$year, \"Q\", control$quarter)), \"\\n\")\n",
    "\n",
    "# Check for missing values in key columns\n",
    "missing_counts <- colSums(is.na(control))\n",
    "if (any(missing_counts > 0)) {\n",
    "    cat(\"\\nMissing values found in the following columns:\\n\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "} else {\n",
    "    cat(\"\\nNo missing values in the merged dataset\\n\")\n",
    "}\n",
    "\n",
    "# Display the first few rows of the merged dataset\n",
    "head(control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85021e6",
   "metadata": {},
   "source": [
    "## Panel preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df9218",
   "metadata": {},
   "source": [
    "### Variable Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3335276",
   "metadata": {},
   "source": [
    "#### GFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64fd7d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Process VIX for quarter-end values\n",
    "vix$year <- as.integer(format(vix$date, \"%Y\"))\n",
    "vix$month <- as.integer(format(vix$date, \"%m\"))\n",
    "vix$quarter <- ceiling(vix$month / 3)\n",
    "\n",
    "# Find last trading day of each quarter\n",
    "vix_quarter_end <- aggregate(\n",
    "    cbind(date, close) ~ year + quarter, \n",
    "    data = vix, \n",
    "    FUN = function(x) x[which.max(as.Date(x))]\n",
    ")\n",
    "vix_quarter_end$month <- vix_quarter_end$quarter * 3  # Last month of quarter\n",
    "\n",
    "# 2. Process VIX for quarterly averages\n",
    "vix_quarterly_avg <- aggregate(close ~ year + quarter, data = vix, FUN = mean)\n",
    "vix_quarterly_avg$month <- vix_quarterly_avg$quarter * 3\n",
    "\n",
    "# 3. Process GFCF for quarter-end values\n",
    "gfcf_quarter_end <- gfcf[gfcf$month %in% c(3, 6, 9, 12), ]\n",
    "gfcf_quarter_end$quarter <- ceiling(gfcf_quarter_end$month / 3)\n",
    "\n",
    "# Calculate logs and first differences\n",
    "# 1. VIX quarter-end\n",
    "vix_quarter_end$log_vix <- log(vix_quarter_end$close)\n",
    "vix_quarter_end <- vix_quarter_end[order(vix_quarter_end$year, vix_quarter_end$quarter), ]\n",
    "vix_quarter_end$log_diff <- c(NA, diff(vix_quarter_end$log_vix))\n",
    "\n",
    "# 2. VIX quarterly average\n",
    "vix_quarterly_avg$log_vix_avg <- log(vix_quarterly_avg$close)\n",
    "vix_quarterly_avg <- vix_quarterly_avg[order(vix_quarterly_avg$year, vix_quarterly_avg$quarter), ]\n",
    "vix_quarterly_avg$log_diff_avg <- c(NA, diff(vix_quarterly_avg$log_vix_avg))\n",
    "\n",
    "# 3. GFCF quarter-end - CHANGED: using direct difference instead of log difference\n",
    "gfcf_quarter_end <- gfcf_quarter_end[order(gfcf_quarter_end$year, gfcf_quarter_end$quarter), ]\n",
    "gfcf_quarter_end$diff_gfcf <- c(NA, diff(gfcf_quarter_end$gfcf))\n",
    "\n",
    "# Merge indicators using inner joins to keep only periods with all data available\n",
    "# First merge VIX quarter-end with VIX quarterly average\n",
    "gfc_indicators <- merge(\n",
    "    vix_quarter_end[, c(\"year\", \"quarter\", \"month\", \"log_diff\")],\n",
    "    vix_quarterly_avg[, c(\"year\", \"quarter\", \"log_diff_avg\")],\n",
    "    by = c(\"year\", \"quarter\"),\n",
    "    all = FALSE  # Inner join\n",
    ")\n",
    "\n",
    "# Then merge with GFCF quarter-end - CHANGED: using diff_gfcf instead of log_diff_gfcf\n",
    "gfc_indicators <- merge(\n",
    "    gfc_indicators,\n",
    "    gfcf_quarter_end[, c(\"year\", \"quarter\", \"diff_gfcf\")],\n",
    "    by = c(\"year\", \"quarter\"),\n",
    "    all = FALSE  # Inner join\n",
    ")\n",
    "\n",
    "# Sort chronologically\n",
    "gfc_indicators <- gfc_indicators[order(gfc_indicators$year, gfc_indicators$quarter), ]\n",
    "\n",
    "# Add lagged values (for t-1 period)\n",
    "gfc_indicators$log_diff_lag1 <- c(NA, gfc_indicators$log_diff[-nrow(gfc_indicators)])\n",
    "gfc_indicators$log_diff_avg_lag1 <- c(NA, gfc_indicators$log_diff_avg[-nrow(gfc_indicators)])\n",
    "gfc_indicators$diff_gfcf_lag1 <- c(NA, gfc_indicators$diff_gfcf[-nrow(gfc_indicators)])\n",
    "\n",
    "# Add date column for easier merging\n",
    "gfc_indicators$date <- as.Date(paste(gfc_indicators$year, gfc_indicators$month, \"01\", sep = \"-\"))\n",
    "\n",
    "gfc_indicators <- gfc_indicators[, c(\"year\", \"month\", \"quarter\", \"log_diff_lag1\", \"log_diff_avg_lag1\", \"diff_gfcf_lag1\")]\n",
    "\n",
    "# Remove rows with NA for lagged values (first period)\n",
    "gfc_indicators <- gfc_indicators[!is.na(gfc_indicators$log_diff_lag1), ]\n",
    "\n",
    "# Display summary\n",
    "cat(\"GFC indicators calculated for\", nrow(gfc_indicators), \"quarters\\n\")\n",
    "head(gfc_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b6259",
   "metadata": {},
   "source": [
    "#### Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a998fb8",
   "metadata": {},
   "source": [
    "- Calculate the volatility as the standard deviation of return rate cycle component within a 48-month (4-year) rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b325607",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(zoo)\n",
    "\n",
    "# Create a new dataframe to store the volatilities with year and month from cycle\n",
    "volatility <- data.frame(year = cycle$year, month = cycle$month)\n",
    "\n",
    "# Set the window size for rolling volatility calculation\n",
    "window_size <- 48 # 48 months\n",
    "\n",
    "# Ensure data is sorted chronologically\n",
    "cycle <- cycle[order(cycle$year, cycle$month), ]\n",
    "volatility <- volatility[order(volatility$year, volatility$month), ]\n",
    "\n",
    "# Calculate simple rolling standard deviation for each stock\n",
    "for (id in stock_id) {\n",
    "    # Use rollapply with standard deviation function\n",
    "    vol <- rollapply(cycle[,id], width=window_size, \n",
    "                    FUN=function(x) sd(x, na.rm=TRUE), \n",
    "                    fill=NA, align=\"right\")\n",
    "    volatility[,id] <- vol\n",
    "}\n",
    "\n",
    "# Remove rows with NA values (first 47 observations due to the rolling window)\n",
    "volatility <- na.omit(volatility)\n",
    "\n",
    "# Add quarter column\n",
    "volatility$quarter <- ceiling(volatility$month / 3)  # Calculate quarter from month\n",
    "\n",
    "# Reorder columns to put year, quarter, month first, followed by stock columns\n",
    "volatility <- volatility[, c(\"year\", \"quarter\", \"month\", stock_id)]\n",
    "\n",
    "# Display the first few rows of the volatility dataframe\n",
    "head(volatility)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302ea7b",
   "metadata": {},
   "source": [
    "- Convert the original volatility table from wide to long form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b3b1f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "\n",
    "# Filter the volatility data to keep only quarterly data (March, June, September, December)\n",
    "volatility_long <- volatility\n",
    "\n",
    "# Reshape from wide to long format\n",
    "volatility_molten <- melt(\n",
    "    volatility_long,\n",
    "    id.vars = c(\"year\", \"month\", \"quarter\"),\n",
    "    measure.vars = stock_id,\n",
    "    variable.name = \"stkcd\",\n",
    "    value.name = \"volatility\"\n",
    ")\n",
    "\n",
    "# Ensure stkcd is character type for consistent merging\n",
    "volatility_molten$stkcd <- as.character(volatility_molten$stkcd)\n",
    "\n",
    "# Create a new dataframe with selected columns\n",
    "volatility <- volatility_molten[, c(\"stkcd\", \"year\", \"quarter\", \"month\", \"volatility\")]\n",
    "\n",
    "# Check for time series continuity\n",
    "# Count observations per stock\n",
    "obs_per_stock <- table(volatility$stkcd)\n",
    "cat(\"Observations per stock - summary:\\n\")\n",
    "print(summary(as.numeric(obs_per_stock)))\n",
    "\n",
    "# Check if all stocks have the same number of time periods\n",
    "expected_periods <- length(unique(paste(volatility$year, volatility$month, sep=\"-\")))\n",
    "cat(\"\\nExpected number of periods per stock:\", expected_periods, \"\\n\")\n",
    "cat(\"Number of stocks with complete data:\", sum(obs_per_stock == expected_periods), \n",
    "    \"out of\", length(obs_per_stock), \"\\n\")\n",
    "\n",
    "# Check for any stocks with incomplete time series\n",
    "if(any(obs_per_stock != expected_periods)) {\n",
    "  cat(\"\\nStocks with incomplete time series:\\n\")\n",
    "  incomplete_stocks <- names(obs_per_stock)[obs_per_stock != expected_periods]\n",
    "  print(data.frame(\n",
    "    Stock = incomplete_stocks,\n",
    "    Observations = as.numeric(obs_per_stock[incomplete_stocks]),\n",
    "    Missing = expected_periods - as.numeric(obs_per_stock[incomplete_stocks])\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Display the first few rows of the melted data\n",
    "head(volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3010a6",
   "metadata": {},
   "source": [
    "- Take logarithm of volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323601e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the volatility dataframe for log transformation\n",
    "volatility_log <- volatility\n",
    "\n",
    "# Check for non-positive values in the volatility column\n",
    "non_positive_count <- sum(volatility_log$volatility <= 0, na.rm = TRUE)\n",
    "if (non_positive_count > 0) {\n",
    "    cat(\"Warning:\", non_positive_count, \"non-positive volatility values found.\\n\")\n",
    "    cat(\"Adding a small constant before log transformation to handle zeros/negatives.\\n\")\n",
    "    \n",
    "    # Add a small constant to handle zeros and negative values\n",
    "    min_positive <- min(volatility_log$volatility[volatility_log$volatility > 0], na.rm = TRUE)\n",
    "    offset <- min_positive * 0.001  # Small fraction of the minimum positive value\n",
    "    volatility_log$volatility <- volatility_log$volatility + offset\n",
    "}\n",
    "\n",
    "# Apply log transformation\n",
    "volatility_log$volatility <- log(volatility_log$volatility)\n",
    "\n",
    "# Verify the transformation\n",
    "cat(\"Volatility summary before log transformation:\\n\")\n",
    "print(summary(volatility$volatility))\n",
    "cat(\"\\nVolatility summary after log transformation:\\n\")\n",
    "print(summary(volatility_log$volatility))\n",
    "\n",
    "# Update the volatility dataframe with log-transformed values\n",
    "volatility <- volatility_log\n",
    "\n",
    "# Display the first few rows of the transformed dataframe\n",
    "head(volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f259d8e",
   "metadata": {},
   "source": [
    "- Remove stocks incurred extreme volatility within observation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ac70b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load required libraries for normality tests and statistics\n",
    "library(moments)  # For skewness and kurtosis functions\n",
    "\n",
    "# Initialize an empty vector to store outlier stock IDs\n",
    "outlier_stocks <- character(0)\n",
    "\n",
    "# Function to measure distance from normal distribution\n",
    "# Lower value = closer to normal distribution\n",
    "normality_distance <- function(x) {\n",
    "  # Calculate absolute skewness (0 for normal) and excess kurtosis (0 for normal)\n",
    "  sk <- abs(skewness(x, na.rm = TRUE))\n",
    "  kt <- abs(kurtosis(x, na.rm = TRUE) - 3)  # Normal kurtosis is 3\n",
    "  \n",
    "  # Combined distance metric\n",
    "  return(sk + kt/2)  # Weighting kurtosis less as it's more sensitive\n",
    "}\n",
    "\n",
    "# Get unique year-month combinations\n",
    "periods <- unique(volatility[, c(\"year\", \"month\")])\n",
    "\n",
    "# For each year-month period, transform distribution toward normal\n",
    "for (i in 1:nrow(periods)) {\n",
    "  curr_year <- periods$year[i]\n",
    "  curr_month <- periods$month[i]\n",
    "  \n",
    "  # Get data for this period\n",
    "  period_data <- volatility[volatility$year == curr_year & volatility$month == curr_month, ]\n",
    "  \n",
    "  # Skip if too few stocks for meaningful distribution shaping\n",
    "  if (nrow(period_data) < 10) next\n",
    "  \n",
    "  # Calculate initial normality distance\n",
    "  initial_distance <- normality_distance(period_data$volatility)\n",
    "  \n",
    "  # Prepare working copy of data\n",
    "  working_data <- period_data\n",
    "  period_stocks <- working_data$stkcd\n",
    "  period_values <- working_data$volatility\n",
    "  \n",
    "  # Set maximum percentage of stocks to remove (to avoid over-pruning)\n",
    "  max_remove_pct <- 0.1  # 10% maximum removal\n",
    "  max_removals <- floor(length(period_stocks) * max_remove_pct)\n",
    "  \n",
    "  # Iteratively remove extreme values\n",
    "  removals <- 0\n",
    "  period_outliers <- character(0)\n",
    "  \n",
    "  while (removals < max_removals) {\n",
    "    # Find current mean and standard deviation\n",
    "    curr_mean <- mean(period_values, na.rm = TRUE)\n",
    "    curr_sd <- sd(period_values, na.rm = TRUE)\n",
    "    \n",
    "    # Calculate z-scores to identify extremes\n",
    "    z_scores <- abs((period_values - curr_mean) / curr_sd)\n",
    "    \n",
    "    # Find the most extreme value (highest z-score)\n",
    "    max_idx <- which.max(z_scores)\n",
    "    \n",
    "    # Temporarily remove it\n",
    "    test_values <- period_values[-max_idx]\n",
    "    \n",
    "    # Check if removal improves normality\n",
    "    new_distance <- normality_distance(test_values)\n",
    "    \n",
    "    # Stop if no improvement or minimal improvement\n",
    "    if (new_distance >= initial_distance || (initial_distance - new_distance) < 0.05) {\n",
    "      break\n",
    "    }\n",
    "    \n",
    "    # Otherwise, keep this removal\n",
    "    period_outliers <- c(period_outliers, period_stocks[max_idx])\n",
    "    period_stocks <- period_stocks[-max_idx]\n",
    "    period_values <- test_values\n",
    "    removals <- removals + 1\n",
    "    initial_distance <- new_distance\n",
    "  }\n",
    "  \n",
    "  # Add to our master list of outlier stocks\n",
    "  outlier_stocks <- unique(c(outlier_stocks, period_outliers))\n",
    "  \n",
    "  # Report progress if significant removals made\n",
    "  if (length(period_outliers) > 0) {\n",
    "    cat(sprintf(\"Period %d-%02d: Removed %d stocks to improve normality\\n\", \n",
    "                curr_year, curr_month, length(period_outliers)))\n",
    "  }\n",
    "}\n",
    "\n",
    "# Print the stocks to be removed\n",
    "cat(\"Removing\", length(outlier_stocks), \"stocks to make distributions more normal\\n\")\n",
    "\n",
    "# Create a filtered version of the volatility dataframe\n",
    "volatility_filtered <- volatility[!volatility$stkcd %in% outlier_stocks, ]\n",
    "\n",
    "# Update stock_id list to exclude outlier stocks\n",
    "stock_id <- stock_id[!stock_id %in% outlier_stocks]\n",
    "\n",
    "# Print summary statistics before and after filtering\n",
    "cat(\"\\nBefore filtering:\", nrow(volatility), \"observations,\", length(unique(volatility$stkcd)), \"stocks\\n\")\n",
    "cat(\"After filtering:\", nrow(volatility_filtered), \"observations,\", length(unique(volatility_filtered$stkcd)), \"stocks\\n\")\n",
    "\n",
    "# Update the main volatility dataframe\n",
    "volatility <- volatility_filtered\n",
    "\n",
    "# Display the first few rows of the filtered dataframe\n",
    "head(volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e142ec",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Filter for quarter-end volatility (month is 3, 6, 9, or 12)\n",
    "volatility_quarter_end <- volatility[volatility$month %in% c(3, 6, 9, 12), ]\n",
    "\n",
    "# 2. Calculate quarterly average volatility for each stock\n",
    "volatility_avg <- aggregate(\n",
    "    volatility ~ stkcd + year + quarter,\n",
    "    data = volatility,\n",
    "    FUN = mean\n",
    ")\n",
    "\n",
    "# 3. Merge quarter-end data with quarterly average\n",
    "volatility_quarterly <- merge(\n",
    "    volatility_quarter_end,\n",
    "    volatility_avg[, c(\"stkcd\", \"year\", \"quarter\", \"volatility\")],\n",
    "    by = c(\"stkcd\", \"year\", \"quarter\"),\n",
    "    all.x = TRUE,\n",
    "    suffixes = c(\"_end\", \"_avg\")\n",
    ")\n",
    "\n",
    "# 4. Create a clean dataframe with the required columns\n",
    "volatility <- volatility_quarterly[, c(\"stkcd\", \"year\", \"quarter\", \"month\", \"volatility_end\", \"volatility_avg\")]\n",
    "\n",
    "\n",
    "# Report the dimensions of the new dataset\n",
    "cat(\"New quarterly volatility dataset has\", nrow(volatility), \"observations for\", \n",
    "        length(unique(volatility$stkcd)), \"stocks\\n\")\n",
    "\n",
    "# Display the first few rows of the updated volatility dataframe\n",
    "head(volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153fc86",
   "metadata": {},
   "source": [
    "- Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5c0df",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "library(ggplot2)\n",
    "library(ggridges)\n",
    "library(dplyr)\n",
    "\n",
    "# Create a copy of the volatility dataframe for visualization\n",
    "vol_viz <- volatility\n",
    "\n",
    "# Create a proper year-quarter factor for chronological ordering\n",
    "vol_viz$yearquarter <- paste0(vol_viz$year, \"-Q\", vol_viz$quarter)\n",
    "\n",
    "# Create a chronologically ordered factor for the y-axis\n",
    "unique_yearquarters <- unique(vol_viz$yearquarter[order(vol_viz$year, vol_viz$quarter)])\n",
    "vol_viz$yearquarter <- factor(vol_viz$yearquarter, levels = unique_yearquarters)\n",
    "\n",
    "# Ridge plot of volatility distributions over time\n",
    "ggplot(vol_viz, aes(x = volatility_end, y = yearquarter, fill = ..x..)) +\n",
    "  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01, alpha = 0.8) +\n",
    "  scale_fill_viridis_c(name = \"Volatility\", option = \"C\") +\n",
    "  labs(title = \"Distribution of End-of-Quarter Stock Volatility Over Time\",\n",
    "       x = \"Volatility (End of Quarter)\",\n",
    "       y = \"Year-Quarter\") +\n",
    "  theme_ridges(font_size = 10, grid = TRUE) +\n",
    "  theme(\n",
    "    legend.position = \"right\",\n",
    "    axis.text.y = element_text(size = 8, hjust = 1),\n",
    "    plot.title = element_text(hjust = 0.5),\n",
    "    panel.spacing.y = unit(0.5, \"lines\")\n",
    "  )\n",
    "\n",
    "# Create a summary of volatility measures for time series visualization\n",
    "vol_summary <- vol_viz %>%\n",
    "  group_by(year, quarter, yearquarter) %>%\n",
    "  summarize(\n",
    "    median_vol = median(volatility_end, na.rm = TRUE),\n",
    "    q25 = quantile(volatility_end, 0.25, na.rm = TRUE),\n",
    "    q75 = quantile(volatility_end, 0.75, na.rm = TRUE),\n",
    "    .groups = \"drop\"\n",
    "  )\n",
    "\n",
    "# Convert yearquarter to a factor with same ordering\n",
    "vol_summary$yearquarter <- factor(vol_summary$yearquarter, levels = unique_yearquarters)\n",
    "\n",
    "# Time series plot showing median volatility and IQR\n",
    "ggplot(vol_summary, aes(x = as.numeric(yearquarter), y = median_vol)) +\n",
    "  geom_line(size = 1, color = \"darkblue\") +\n",
    "  geom_ribbon(aes(ymin = q25, ymax = q75), alpha = 0.3, fill = \"lightblue\") +\n",
    "  labs(title = \"Median Stock Volatility Over Time (with IQR)\",\n",
    "       x = \"Time\", \n",
    "       y = \"Volatility (End of Quarter)\") +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5),\n",
    "    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)\n",
    "  ) +\n",
    "  # Add custom x-axis labels with appropriate gaps\n",
    "  scale_x_continuous(\n",
    "    breaks = seq(1, length(unique_yearquarters), by = 4),\n",
    "    labels = unique_yearquarters[seq(1, length(unique_yearquarters), by = 4)]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4d2d2",
   "metadata": {},
   "source": [
    "### Data Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f90a3ea",
   "metadata": {},
   "source": [
    "- 1. Merge volatility with GFC indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6ab70",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "panel_data <- merge(\n",
    "    volatility,\n",
    "    gfc_indicators,\n",
    "    by = c(\"year\", \"month\", \"quarter\"),\n",
    "    all.x = TRUE\n",
    ")\n",
    "\n",
    "# Show a sample of the final merged dataset\n",
    "head(panel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5672975",
   "metadata": {},
   "source": [
    "- 2. Merge with control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f1b9e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "panel_data <- merge(\n",
    "    panel_data,\n",
    "    control,\n",
    "    by = c(\"stkcd\", \"year\", \"quarter\", \"month\"),\n",
    "    all.x = FALSE\n",
    ")\n",
    "\n",
    "# Display column names to verify what's available\n",
    "cat(\"Columns in merged panel_data:\\n\")\n",
    "print(names(panel_data))\n",
    "\n",
    "# Create factor variables for fixed effects\n",
    "panel_data$quarter <- factor(panel_data$quarter)  # Quarter fixed effects\n",
    "\n",
    "# Check for missing data in key columns\n",
    "missing_counts <- colSums(is.na(panel_data))\n",
    "cat(\"Missing value counts in final panel dataset:\\n\")\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Show a sample of the final merged dataset\n",
    "head(panel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c932dcf7",
   "metadata": {},
   "source": [
    "- 3. Merge with Capital Inflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cf4ff",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Merge panel_data with inflow data based on year and quarter\n",
    "# Using inflow data read previously\n",
    "panel_data <- merge(\n",
    "    panel_data,\n",
    "    inflow[, c(\"year\", \"quarter\", \"month\", \"inflow\")],\n",
    "    by = c(\"year\", \"quarter\", \"month\"),\n",
    "    all.x = TRUE\n",
    ")\n",
    "\n",
    "# Check for missing values in the inflow variable\n",
    "cat(\"Missing values in inflow variable:\", sum(is.na(panel_data$inflow)), \"\\n\")\n",
    "\n",
    "# Display dimensions of the enhanced panel dataset\n",
    "cat(\"\\nPanel dataset dimensions after adding inflow data: \", \n",
    "    nrow(panel_data), \"observations,\", ncol(panel_data), \"variables\\n\")\n",
    "\n",
    "# Display a sample of the merged data\n",
    "head(panel_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8425c0d",
   "metadata": {},
   "source": [
    "- 4. Merge with EPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86396a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Merge panel_data with EPU data based on year, quarter, and month\n",
    "panel_data <- merge(\n",
    "    panel_data,\n",
    "    epu[, c(\"year\", \"quarter\", \"month\", \"log_epu\")],\n",
    "    by = c(\"year\", \"quarter\", \"month\"),\n",
    "    all.x = TRUE\n",
    ")\n",
    "\n",
    "# Check for missing values in the EPU variable\n",
    "cat(\"Missing values in EPU variable:\", sum(is.na(panel_data$log_epu)), \"\\n\")\n",
    "\n",
    "# Display dimensions of the enhanced panel dataset\n",
    "cat(\"\\nPanel dataset dimensions after adding EPU data: \", \n",
    "    nrow(panel_data), \"observations,\", ncol(panel_data), \"variables\\n\")\n",
    "\n",
    "# Display a sample of the merged data with EPU\n",
    "head(panel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd364e5",
   "metadata": {},
   "source": [
    "- 5. Convert to panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7c65a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Create panel data structure\n",
    "panel_data <- pdata.frame(panel_data, index = c(\"stkcd\", \"year\", \"quarter\"))\n",
    "\n",
    "# Basic panel information\n",
    "cat(\"Panel Data Structure:\\n\")\n",
    "cat(\"-------------------\\n\")\n",
    "cat(\"Total observations:\", nrow(panel_data), \"\\n\")\n",
    "cat(\"Number of stocks:\", length(unique(panel_data$stkcd)), \"\\n\")\n",
    "cat(\"Number of time periods:\", length(unique(paste(panel_data$year, panel_data$quarter))), \"\\n\")\n",
    "\n",
    "# Check if panel is balanced\n",
    "pdim <- pdim(panel_data)\n",
    "cat(\"\\nPanel Dimensions:\\n\")\n",
    "print(pdim)\n",
    "\n",
    "if (pdim$balanced) {\n",
    "    cat(\"\\nThe panel is balanced.\\n\")\n",
    "} else {\n",
    "    cat(\"\\nThe panel is unbalanced.\\n\")\n",
    "    # Calculate distribution of observations per stock\n",
    "    obs_per_stock <- table(index(panel_data, \"id\"))\n",
    "    cat(\"Observation distribution per stock:\\n\")\n",
    "    print(summary(as.numeric(obs_per_stock)))\n",
    "}\n",
    "\n",
    "# Display the first few rows\n",
    "head(panel_data)\n",
    "tail(panel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5390ef6",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9cb550",
   "metadata": {},
   "source": [
    "1. Main\n",
    "\n",
    "- $\\text{Volatility}_{i,t} = \\alpha + \\beta \\text{GFC}_{t-1} + \\sum \\text{Control} + \\sum \\text{FE} + \\epsilon_{i,t}$\n",
    "\n",
    "|Model|Volatility|GFC (lagged 1 period)|Firm|Macro|FE(S)|FE(Q)|FE(P)|FE(I)| \n",
    "|:-|:-|:-|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|a|$\\ln(\\text{Volatility}_{\\text{end of quarter }t})$|$\\Delta\\ln(\\text{VIX}_{\\text{end of quarter }t})$|||||||\n",
    "|a1|$\\ln(\\text{Volatility}_{\\text{end of quarter }t})$|$\\Delta\\ln(\\text{VIX}_{\\text{end of quarter }t})$|||||||\n",
    "\n",
    "2. Robustness Test\n",
    "\n",
    "|Model|Volatility|GFC (lagged 1 period)|Firm|Macro|FE(S)|FE(Q)|FE(P)|FE(I)|\n",
    "|:-|:-|:-|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|a2|$\\ln(\\overline{\\text{Volatility}_{\\text{quarter }t}})$|$\\Delta\\ln(\\overline{\\text{VIX}_{\\text{quarter }t}})$|||||||\n",
    "|a3|$\\ln(\\text{Volatility}_{\\text{end of quarter }t})$|$\\Delta\\text{GFCF}_{\\text{end of quarter }t}$|||||||\n",
    "|a4|$\\ln(\\text{Volatility}_{\\text{end of quarter }t})$|$\\Delta\\ln(\\text{VIX}_{\\text{end of quarter }t})$|||||||\n",
    "\n",
    "3. Channel Verification: `GFC` >> Channel >> `Volatility`\n",
    "\n",
    "    1. $\\text{Channel}_{i,t} = \\alpha + \\beta \\text{GFC}_{t-1} + \\sum \\text{Control} + \\sum \\text{FE} + \\epsilon_{i,t}$\n",
    "    2. $\\text{Volatility}_{i,t} = \\alpha + \\beta \\text{Channel}_{t} + \\sum \\text{Control} + \\sum \\text{FE} + \\epsilon_{i,t}$\n",
    "\n",
    "|Model|Volatility|Channel|GFC (lagged 1 period)|Firm|Macro|FE(S)|FE(Q)|\n",
    "|:-|:-|:-|:-|:-:|:-:|:-:|:-:|\n",
    "|b1|$\\ln(\\text{Volatility}_{\\text{end of quarter }t})$|`inflow`|$\\Delta\\ln(\\text{VIX}_{\\text{end of quarter }t})$|||||\n",
    "|b2|$\\ln(\\text{Volatility}_{\\text{end of quarter }t})$|`epu`|$\\Delta\\ln(\\text{VIX}_{\\text{end of quarter }t})$|||||\n",
    "\n",
    "where:\n",
    "\n",
    "||Channels|\n",
    "|:-|:-:|\n",
    "|`inflow`| |\n",
    "|`epu`| |\n",
    "\n",
    "and\n",
    "\n",
    "||Control Variables|\n",
    "|:-|:-:|\n",
    "|Firm|`Size`, `Lev`, `ROA`, `BM`, `Board`, `Top1`|\n",
    "|Macro|`GDP Growth Rate`, `Money Supply Growth Rate`|\n",
    "\n",
    "also\n",
    "\n",
    "||Fixed Effects|\n",
    "|:-|:-:|\n",
    "|FE(S)|`Individual (stock)`|\n",
    "|FE(Q)|`Quarter`|\n",
    "|FE(P)|`Province`|\n",
    "|FE(I)|`Industry`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0ed5c",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcf62b",
   "metadata": {},
   "source": [
    "### Model a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa86ef",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_a <- plm(volatility_end ~ log_diff_lag1 + I(log_diff_lag1^2) + I(log_diff_lag1^3) + quarter, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_a)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_a, vcov = vcovHC(model_a, type = \"HC1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc3c4e",
   "metadata": {},
   "source": [
    "### Model a1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e0162",
   "metadata": {},
   "source": [
    "#### Panel Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd2fe4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_a1 <- plm(volatility_end ~ log_diff_lag1 + I(log_diff_lag1^2) + I(log_diff_lag1^3) +  Size + Lev + ROA + BM + Board + Top1 + quarter + gdp_growth_rate + m2_growth_rate, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_a1)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_a1, vcov = vcovHC(model_a1, type = \"HC1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96911fde",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\", mar=c(5, 4, 4, 2) + 0.1)\n",
    "\n",
    "# Extract coefficients for log_diff_lag1 and its higher order terms\n",
    "coefficients <- coef(model_a1)\n",
    "b1 <- coefficients[\"log_diff_lag1\"]\n",
    "b2 <- coefficients[\"I(log_diff_lag1^2)\"]\n",
    "b3 <- coefficients[\"I(log_diff_lag1^3)\"]\n",
    "\n",
    "# Use the range from the original gfc_indicators dataframe\n",
    "min_value <- min(gfc_indicators$log_diff_lag1, na.rm = TRUE)\n",
    "max_value <- max(gfc_indicators$log_diff_lag1, na.rm = TRUE)\n",
    "log_diff_values <- seq(min_value, max_value, length.out = 100)\n",
    "\n",
    "# Calculate predicted volatility for each value (partial effect, ignoring other variables)\n",
    "predicted_volatility <- b1 * log_diff_values + \n",
    "                        b2 * log_diff_values^2 + \n",
    "                        b3 * log_diff_values^3\n",
    "\n",
    "# Get the variance-covariance matrix for the coefficients\n",
    "vcov_matrix <- vcovHC(model_a1, type = \"HC1\") # Using robust variance-covariance matrix\n",
    "\n",
    "# Extract the rows/columns for our parameters of interest\n",
    "param_indices <- which(names(coefficients) %in% c(\"log_diff_lag1\", \"I(log_diff_lag1^2)\", \"I(log_diff_lag1^3)\"))\n",
    "vcov_subset <- vcov_matrix[param_indices, param_indices]\n",
    "\n",
    "# Calculate standard error for each prediction point\n",
    "se_predictions <- numeric(length(log_diff_values))\n",
    "for (i in 1:length(log_diff_values)) {\n",
    "    # Create X matrix for this prediction point\n",
    "    X_i <- c(log_diff_values[i], log_diff_values[i]^2, log_diff_values[i]^3)\n",
    "    \n",
    "    # Calculate variance of prediction\n",
    "    se_predictions[i] <- sqrt(t(X_i) %*% vcov_subset %*% X_i)\n",
    "}\n",
    "\n",
    "# Calculate confidence intervals (95%)\n",
    "confidence_level <- 0.95\n",
    "t_value <- qt((1 + confidence_level) / 2, df = df.residual(model_a1))\n",
    "upper_ci <- predicted_volatility + t_value * se_predictions\n",
    "lower_ci <- predicted_volatility - t_value * se_predictions\n",
    "\n",
    "# Create the plot\n",
    "plot(log_diff_values, predicted_volatility, type = \"l\", lwd = 2, col = \"blue\",\n",
    "     xlab = \"Log Difference of VIX (lagged 1 period)\",\n",
    "     ylab = \"Predicted Volatility\",\n",
    "     main = \"Effect of VIX Changes on Stock Volatility\",\n",
    "     ylim = range(c(lower_ci, upper_ci)))\n",
    "\n",
    "# Add confidence intervals\n",
    "polygon(c(log_diff_values, rev(log_diff_values)), \n",
    "        c(lower_ci, rev(upper_ci)), \n",
    "        col = rgb(0, 0, 1, 0.2), border = NA)\n",
    "\n",
    "# Add the CI lines\n",
    "lines(log_diff_values, upper_ci, lty = 2, col = \"blue\")\n",
    "lines(log_diff_values, lower_ci, lty = 2, col = \"blue\")\n",
    "\n",
    "# Add a vertical line at x = 0 for reference\n",
    "abline(v = 0, lty = 2, col = \"darkgray\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "grid()\n",
    "\n",
    "# Add a legend\n",
    "legend(\"bottomleft\", \n",
    "       legend = c(\"Predicted Effect\", \"95% Confidence Interval\"),\n",
    "       col = c(\"blue\", rgb(0, 0, 1, 0.2)),\n",
    "       lty = c(1, NA), \n",
    "       lwd = c(2, NA),\n",
    "       fill = c(NA, rgb(0, 0, 1, 0.2)),\n",
    "       border = c(NA, NA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb2807",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c301f",
   "metadata": {},
   "source": [
    "- Hausman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f75477",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# First estimate a random effects version of model_a1\n",
    "model_a1_re <- plm(volatility_end ~ log_diff_lag1 + I(log_diff_lag1^2) + I(log_diff_lag1^3) +  \n",
    "                   Size + Lev + ROA + BM + Board + Top1 + quarter + \n",
    "                   gdp_growth_rate + m2_growth_rate, \n",
    "                   data = panel_data, \n",
    "                   model = \"random\")\n",
    "\n",
    "# Perform Hausman test\n",
    "hausman_test <- phtest(model_a1, model_a1_re)\n",
    "\n",
    "# Print the results\n",
    "print(hausman_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2e5fa",
   "metadata": {},
   "source": [
    "- Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b4f63",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract fitted values and residuals\n",
    "fitted_values_model_a1 <- as.numeric(fitted(model_a1))\n",
    "residuals_model_a1 <- as.numeric(residuals(model_a1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61213ef",
   "metadata": {},
   "source": [
    "- Fitted vs Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d881ee",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\")\n",
    "\n",
    "# Create a scatter plot of fitted values vs residuals\n",
    "plot(fitted_values_model_a1, residuals_model_a1, \n",
    "    pch = 20, col = \"darkblue\",\n",
    "    xlab = \"Fitted Values\", ylab = \"Residuals\",\n",
    "    main = \"Residual Diagnostic Plot for Model a1\")\n",
    "\n",
    "# Add a horizontal line at y = 0\n",
    "abline(h = 0, col = \"red\", lwd = 2)\n",
    "\n",
    "# Add a smoothed line to detect patterns\n",
    "# Create a data frame for loess model\n",
    "loess_data <- data.frame(x = fitted_values_model_a1, y = residuals_model_a1)\n",
    "smooth_line <- loess(y ~ x, data = loess_data, span = 0.75)\n",
    "\n",
    "# Sort the data for smooth line plotting\n",
    "sorted_data <- loess_data[order(loess_data$x),]\n",
    "# Predict using the sorted data\n",
    "smooth_pred <- predict(smooth_line, newdata = sorted_data)\n",
    "\n",
    "# Plot the smooth line\n",
    "lines(sorted_data$x, smooth_pred, col = \"cyan\", lwd = 2)\n",
    "\n",
    "# Add a legend\n",
    "legend(\"topright\", \n",
    "      legend = c(\"Residuals\", \"Reference Line\", \"Smooth Line\"),\n",
    "      col = c(\"darkblue\", \"red\", \"cyan\"),\n",
    "      pch = c(20, NA, NA), \n",
    "      lty = c(NA, 1, 1),\n",
    "      lwd = c(NA, 2, 2))\n",
    "\n",
    "# Add grid for better readability\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438e74c",
   "metadata": {},
   "source": [
    "- Q-Q Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26a07c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\")\n",
    "qqnorm(fitted_values_model_a1)\n",
    "qqline(residuals_model_a1, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7058f",
   "metadata": {},
   "source": [
    "## Robustness Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba4cd87",
   "metadata": {},
   "source": [
    "### Model a2\n",
    "\n",
    "> Robustness verification of Model a1 by changing $\\text{Volatility}$ and $\\text{GFC}$ from quarter-end data to quarterly-averaged data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f518c4d",
   "metadata": {},
   "source": [
    "#### Panel Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df296c4c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_a2 <- plm(volatility_avg ~ log_diff_avg_lag1 + I(log_diff_avg_lag1^2) + I(log_diff_avg_lag1^3) + Size + Lev + ROA + BM + Board + Top1 + quarter + gdp_growth_rate + m2_growth_rate, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_a2)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_a2, vcov = vcovHC(model_a2, type = \"HC1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949885a",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926691e",
   "metadata": {},
   "source": [
    "- Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a5157",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract fitted values and residuals\n",
    "fitted_values_model_a2 <- as.numeric(fitted(model_a2))\n",
    "residuals_model_a2 <- as.numeric(residuals(model_a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e35ad",
   "metadata": {},
   "source": [
    "- Fitted vs Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ecdcd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\")\n",
    "\n",
    "# Create a scatter plot of fitted values vs residuals\n",
    "plot(fitted_values_model_a2, residuals_model_a2, \n",
    "    pch = 20, col = \"darkblue\",\n",
    "    xlab = \"Fitted Values\", ylab = \"Residuals\",\n",
    "    main = \"Residual Diagnostic Plot for Model a1\")\n",
    "\n",
    "# Add a horizontal line at y = 0\n",
    "abline(h = 0, col = \"red\", lwd = 2)\n",
    "\n",
    "# Add a smoothed line to detect patterns\n",
    "# Create a data frame for loess model\n",
    "loess_data <- data.frame(x = fitted_values_model_a2, y = residuals_model_a2)\n",
    "smooth_line <- loess(y ~ x, data = loess_data, span = 0.75)\n",
    "\n",
    "# Sort the data for smooth line plotting\n",
    "sorted_data <- loess_data[order(loess_data$x),]\n",
    "# Predict using the sorted data\n",
    "smooth_pred <- predict(smooth_line, newdata = sorted_data)\n",
    "\n",
    "# Plot the smooth line\n",
    "lines(sorted_data$x, smooth_pred, col = \"cyan\", lwd = 2)\n",
    "\n",
    "# Add a legend\n",
    "legend(\"topright\", \n",
    "      legend = c(\"Residuals\", \"Reference Line\", \"Smooth Line\"),\n",
    "      col = c(\"darkblue\", \"red\", \"cyan\"),\n",
    "      pch = c(20, NA, NA), \n",
    "      lty = c(NA, 1, 1),\n",
    "      lwd = c(NA, 2, 2))\n",
    "\n",
    "# Add grid for better readability\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1819e4",
   "metadata": {},
   "source": [
    "- Q-Q Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85951643",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\")\n",
    "qqnorm(fitted_values_model_a2)\n",
    "qqline(residuals_model_a2, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215175dc",
   "metadata": {},
   "source": [
    "### Model a3\n",
    "\n",
    "> Robustness verification of Model a1 by changing the proxy for $\\text{GFC}$ from VIX-based data to GFCF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46045",
   "metadata": {},
   "source": [
    "#### Panel Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa65c30",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_a3 <- plm(volatility_end ~ diff_gfcf_lag1 + I(diff_gfcf_lag1^2) + I(diff_gfcf_lag1^3) + Size + Lev + ROA + BM + Board + Top1 + quarter + gdp_growth_rate + m2_growth_rate, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_a3)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_a3, vcov = vcovHC(model_a3, type = \"HC1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e039f",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2dd84",
   "metadata": {},
   "source": [
    "- Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1423635",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract fitted values and residuals\n",
    "fitted_values_model_a3 <- as.numeric(fitted(model_a3))\n",
    "residuals_model_a3 <- as.numeric(residuals(model_a3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b72c99d",
   "metadata": {},
   "source": [
    "- Fitted vs Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3927bf",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\")\n",
    "\n",
    "# Create a scatter plot of fitted values vs residuals\n",
    "plot(fitted_values_model_a3, residuals_model_a3, \n",
    "    pch = 20, col = \"darkblue\",\n",
    "    xlab = \"Fitted Values\", ylab = \"Residuals\",\n",
    "    main = \"Residual Diagnostic Plot for Model a1\")\n",
    "\n",
    "# Add a horizontal line at y = 0\n",
    "abline(h = 0, col = \"red\", lwd = 2)\n",
    "\n",
    "# Add a smoothed line to detect patterns\n",
    "# Create a data frame for loess model\n",
    "loess_data <- data.frame(x = fitted_values_model_a2, y = residuals_model_a3)\n",
    "smooth_line <- loess(y ~ x, data = loess_data, span = 0.75)\n",
    "\n",
    "# Sort the data for smooth line plotting\n",
    "sorted_data <- loess_data[order(loess_data$x),]\n",
    "# Predict using the sorted data\n",
    "smooth_pred <- predict(smooth_line, newdata = sorted_data)\n",
    "\n",
    "# Plot the smooth line\n",
    "lines(sorted_data$x, smooth_pred, col = \"cyan\", lwd = 2)\n",
    "\n",
    "# Add a legend\n",
    "legend(\"topright\", \n",
    "      legend = c(\"Residuals\", \"Reference Line\", \"Smooth Line\"),\n",
    "      col = c(\"darkblue\", \"red\", \"cyan\"),\n",
    "      pch = c(20, NA, NA), \n",
    "      lty = c(NA, 1, 1),\n",
    "      lwd = c(NA, 2, 2))\n",
    "\n",
    "# Add grid for better readability\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22cb68",
   "metadata": {},
   "source": [
    "- Q-Q Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b4265",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\")\n",
    "qqnorm(fitted_values_model_a3)\n",
    "qqline(residuals_model_a3, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a0ee5",
   "metadata": {},
   "source": [
    "### Model a4\n",
    "\n",
    "> Robustness verification of Model a1 by controlling industrial and provincial fixed effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc37b5",
   "metadata": {},
   "source": [
    "#### Panel Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9963de",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_a4 <- plm(volatility_end ~ log_diff_avg_lag1 + I(log_diff_avg_lag1^2) + I(log_diff_avg_lag1^3) + Size + Lev + ROA + BM + Board + Top1 + quarter + ind + gdp_growth_rate + m2_growth_rate, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_a4)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_a4, vcov = vcovHC(model_a4, type = \"HC1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b00200",
   "metadata": {},
   "source": [
    "## Channel Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07484065",
   "metadata": {},
   "source": [
    "### Model b1\n",
    "\n",
    "> `GFC` >> `International Capital Inflow` >> `Volatility`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c6d89",
   "metadata": {},
   "source": [
    "- `GFC` >> `International Capital Inflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52540ad6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_b1_1 <- plm(inflow ~ log_diff_lag1 + I(log_diff_lag1^2) + I(log_diff_lag1^3) + Size + Lev + ROA + BM + Board + Top1 + quarter + gdp_growth_rate + m2_growth_rate, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_b1_1)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_b1_1, vcov = vcovHC(model_b1_1, type = \"HC1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30baf5d4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\", mar=c(5, 4, 4, 2) + 0.1)\n",
    "\n",
    "# Extract coefficients for log_diff_lag1 and its higher order terms\n",
    "coefficients <- coef(model_b1_1)\n",
    "b1 <- coefficients[\"log_diff_lag1\"]\n",
    "b2 <- coefficients[\"I(log_diff_lag1^2)\"]\n",
    "b3 <- coefficients[\"I(log_diff_lag1^3)\"]\n",
    "\n",
    "# Use the range from the original gfc_indicators dataframe\n",
    "min_value <- min(gfc_indicators$log_diff_lag1, na.rm = TRUE)\n",
    "max_value <- max(gfc_indicators$log_diff_lag1, na.rm = TRUE)\n",
    "log_diff_values <- seq(min_value, max_value, length.out = 100)\n",
    "\n",
    "# Calculate predicted inflow for each value (partial effect, ignoring other variables)\n",
    "predicted_inflow <- b1 * log_diff_values + \n",
    "                    b2 * log_diff_values^2 + \n",
    "                    b3 * log_diff_values^3\n",
    "\n",
    "# Get the variance-covariance matrix for the coefficients\n",
    "vcov_matrix <- vcovHC(model_b1_1, type = \"HC1\") # Using robust variance-covariance matrix\n",
    "\n",
    "# Extract the rows/columns for our parameters of interest\n",
    "param_indices <- which(names(coefficients) %in% c(\"log_diff_lag1\", \"I(log_diff_lag1^2)\", \"I(log_diff_lag1^3)\"))\n",
    "vcov_subset <- vcov_matrix[param_indices, param_indices]\n",
    "\n",
    "# Calculate standard error for each prediction point\n",
    "se_predictions <- numeric(length(log_diff_values))\n",
    "for (i in 1:length(log_diff_values)) {\n",
    "    # Create X matrix for this prediction point\n",
    "    X_i <- c(log_diff_values[i], log_diff_values[i]^2, log_diff_values[i]^3)\n",
    "    \n",
    "    # Calculate variance of prediction\n",
    "    se_predictions[i] <- sqrt(t(X_i) %*% vcov_subset %*% X_i)\n",
    "}\n",
    "\n",
    "# Calculate confidence intervals (95%)\n",
    "confidence_level <- 0.95\n",
    "t_value <- qt((1 + confidence_level) / 2, df = df.residual(model_b1_1))\n",
    "upper_ci <- predicted_inflow + t_value * se_predictions\n",
    "lower_ci <- predicted_inflow - t_value * se_predictions\n",
    "\n",
    "# Create the plot\n",
    "plot(log_diff_values, predicted_inflow, type = \"l\", lwd = 2, col = \"blue\",\n",
    "     xlab = \"Log Difference of VIX (lagged 1 period)\",\n",
    "     ylab = \"Predicted Inflow\",\n",
    "     main = \"Effect of VIX Changes on Fund Inflow\",\n",
    "     ylim = range(c(lower_ci, upper_ci)))\n",
    "\n",
    "# Add confidence intervals\n",
    "polygon(c(log_diff_values, rev(log_diff_values)), \n",
    "        c(lower_ci, rev(upper_ci)), \n",
    "        col = rgb(0, 0, 1, 0.2), border = NA)\n",
    "\n",
    "# Add the CI lines\n",
    "lines(log_diff_values, upper_ci, lty = 2, col = \"blue\")\n",
    "lines(log_diff_values, lower_ci, lty = 2, col = \"blue\")\n",
    "\n",
    "# Add a vertical line at x = 0 for reference\n",
    "abline(v = 0, lty = 2, col = \"darkgray\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "grid()\n",
    "\n",
    "# Add a legend\n",
    "legend(\"bottomleft\", \n",
    "       legend = c(\"Predicted Effect\", \"95% Confidence Interval\"),\n",
    "       col = c(\"blue\", rgb(0, 0, 1, 0.2)),\n",
    "       lty = c(1, NA), \n",
    "       lwd = c(2, NA),\n",
    "       fill = c(NA, rgb(0, 0, 1, 0.2)),\n",
    "       border = c(NA, NA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389fc4c",
   "metadata": {},
   "source": [
    "- `International Capital Inflow` >> `Volatility`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d4858",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_b1_2 <- plm(volatility_end ~ inflow +  Size + Lev + ROA + BM + Board + Top1 + quarter + gdp_growth_rate + m2_growth_rate, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_b1_2)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_b1_2, vcov = vcovHC(model_b1_2, type = \"HC1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f2cf7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "par(bg=\"white\", mar=c(5, 4, 4, 2) + 0.1)\n",
    "\n",
    "# Extract coefficient for inflow\n",
    "coefficient <- coef(model_b1_2)[\"inflow\"]\n",
    "vcov_matrix <- vcovHC(model_b1_2, type = \"HC1\") \n",
    "se <- sqrt(vcov_matrix[\"inflow\", \"inflow\"])\n",
    "\n",
    "# Use the range from panel_data\n",
    "min_value <- quantile(panel_data$inflow, 0.05, na.rm = TRUE)\n",
    "max_value <- quantile(panel_data$inflow, 0.95, na.rm = TRUE)\n",
    "inflow_values <- seq(min_value, max_value, length.out = 100)\n",
    "\n",
    "# Calculate predicted change in volatility across the range\n",
    "effect_values <- coefficient * inflow_values\n",
    "\n",
    "# Calculate confidence intervals\n",
    "t_value <- qt(0.975, df = df.residual(model_b1_2))\n",
    "ci_width <- t_value * se * abs(inflow_values)\n",
    "upper_ci <- effect_values + ci_width\n",
    "lower_ci <- effect_values - ci_width\n",
    "\n",
    "# Create the plot\n",
    "plot(inflow_values, effect_values, type = \"l\", lwd = 2, col = \"blue\",\n",
    "    xlab = \"Fund Inflow\",\n",
    "    ylab = \"Effect on Log Volatility\",\n",
    "    main = \"Effect of Fund Inflow on Stock Volatility\",\n",
    "    ylim = range(c(lower_ci, upper_ci, 0)))  # Include 0 in range\n",
    "\n",
    "# Add confidence intervals\n",
    "polygon(c(inflow_values, rev(inflow_values)), \n",
    "       c(lower_ci, rev(upper_ci)), \n",
    "       col = rgb(0, 0, 1, 0.2), border = NA)\n",
    "\n",
    "# Add the CI lines\n",
    "lines(inflow_values, upper_ci, lty = 2, col = \"blue\")\n",
    "lines(inflow_values, lower_ci, lty = 2, col = \"blue\")\n",
    "\n",
    "# Add a horizontal line at y = 0 for reference\n",
    "abline(h = 0, lty = 2, col = \"red\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "grid()\n",
    "\n",
    "# Add a legend\n",
    "legend(\"bottomleft\", \n",
    "      legend = c(\"Marginal Effect\", \"95% Confidence Interval\", \"Zero Reference\"),\n",
    "      col = c(\"blue\", rgb(0, 0, 1, 0.2), \"red\"),\n",
    "      lty = c(1, NA, 2), \n",
    "      lwd = c(2, NA, 1),\n",
    "      fill = c(NA, rgb(0, 0, 1, 0.2), NA),\n",
    "      border = c(NA, NA, NA))\n",
    "\n",
    "# Add p-value information\n",
    "p_value <- 2 * pt(-abs(coefficient/se), df = df.residual(model_b1_2))\n",
    "p_text <- if (p_value < 0.001) \"p < 0.001\" else paste(\"p =\", round(p_value, 3))\n",
    "mtext(paste(\"Coefficient =\", round(coefficient, 4), \",\", p_text), \n",
    "     side = 3, line = 0.5, col = \"darkblue\", cex = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7142e4e",
   "metadata": {},
   "source": [
    "### Model b2\n",
    "\n",
    "> `GFC` >> `Policy Uncertainty` >> `Volatility`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae298ff0",
   "metadata": {},
   "source": [
    "- `GFC` >> `Policy Uncertainty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5cfc0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_b2_1 <- plm(log_epu ~ log_diff_lag1 + I(log_diff_lag1^2) + I(log_diff_lag1^3) + Size + Lev + ROA + BM + Board + Top1 + quarter + gdp_growth_rate + m2_growth_rate, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_b2_1)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_b2_1, vcov = vcovHC(model_b2_1, type = \"HC1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ae48d",
   "metadata": {},
   "source": [
    "- `Policy Uncertainty` >> `Volatility`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3642ee",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(plm)\n",
    "\n",
    "# Run model with individual fixed effects and quarter dummies\n",
    "model_b2_2 <- plm(volatility_end ~ log_epu +  Size + Lev + ROA + BM + Board + Top1 + quarter + gdp_growth_rate + m2_growth_rate, \n",
    "             data = panel_data, \n",
    "             model = \"within\", \n",
    "             effect = \"individual\")\n",
    "\n",
    "# Display regression results\n",
    "summary(model_b2_2)\n",
    "\n",
    "# Get robust standard errors for both models\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "cat(\"\\nRobust standard errors for two-way fixed effects model:\\n\")\n",
    "coeftest(model_b2_2, vcov = vcovHC(model_b2_2, type = \"HC1\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
